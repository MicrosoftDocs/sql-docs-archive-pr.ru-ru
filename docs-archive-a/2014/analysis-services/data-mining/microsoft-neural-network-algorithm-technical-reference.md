---
title: Технический справочник по алгоритму нейронной сети (Майкрософт) | Документация Майкрософт
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- HIDDEN_NODE_RATIO parameter
- MAXIMUM_INPUT_ATTRIBUTES parameter
- HOLDOUT_PERCENTAGE parameter
- neural network algorithms [Analysis Services]
- output layer [Data Mining]
- neural networks
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- MAXIMUM_STATES parameter
- SAMPLE_SIZE parameter
- hidden layer
- hidden neurons
- input layer [Data Mining]
- activation function [Data Mining]
- Back-Propagated Delta Rule network
- neural network model [Analysis Services]
- coding [Data Mining]
- HOLDOUT_SEED parameter
ms.assetid: b8fac409-e3c0-4216-b032-364f8ea51095
author: minewiskan
ms.author: owend
ms.openlocfilehash: 3c36fd9f3446ddf36da9af7ce58259edbe84c8cf
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 08/04/2020
ms.locfileid: "87654612"
---
# <a name="microsoft-neural-network-algorithm-technical-reference"></a><span data-ttu-id="0f339-102">Технический справочник по алгоритму нейронной сети (Майкрософт)</span><span class="sxs-lookup"><span data-stu-id="0f339-102">Microsoft Neural Network Algorithm Technical Reference</span></span>
  <span data-ttu-id="0f339-103">Алгоритм нейронной сети ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] ) использует сеть в виде *многослойного перцептрона* , также известного под названием *сеть дельта-правила с обратным распространением*, в состав которой может входить до трех слоев нейронов, или *перцептронов*.</span><span class="sxs-lookup"><span data-stu-id="0f339-103">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network uses a *Multilayer Perceptron* network, also called a *Back-Propagated Delta Rule network*, composed of up to three layers of neurons, or *perceptrons*.</span></span> <span data-ttu-id="0f339-104">Такими слоями являются входной слой, необязательный скрытый слой и выходной слой.</span><span class="sxs-lookup"><span data-stu-id="0f339-104">These layers are an input layer, an optional hidden layer, and an output layer.</span></span>  
  
 <span data-ttu-id="0f339-105">В задачи настоящей документации не входит подробное рассмотрение таких нейронных сетей, как многослойные перцептроны.</span><span class="sxs-lookup"><span data-stu-id="0f339-105">A detailed discussion of Multilayer Perceptron neural networks is outside the scope of this documentation.</span></span> <span data-ttu-id="0f339-106">В данном разделе содержится описание базовой реализации алгоритма, в том числе метода, используемого для нормализации входных и выходных значений. Приводится также описание методов выбора компонентов, которые применяются для снижения количества элементов атрибута.</span><span class="sxs-lookup"><span data-stu-id="0f339-106">This topic explains the basic implementation of the algorithm, including the method used to normalize input and output values, and feature selection methods used to reduce attribute cardinality.</span></span> <span data-ttu-id="0f339-107">В данном разделе описываются параметры и другие настройки, с помощью которых можно управлять поведением алгоритма. Приводятся также ссылки на дополнительную информацию о запросах к модели.</span><span class="sxs-lookup"><span data-stu-id="0f339-107">This topic describes the parameters and other settings that can be used to customize the behavior of the algorithm, and provides links to additional information about querying the model.</span></span>  
  
## <a name="implementation-of-the-microsoft-neural-network-algorithm"></a><span data-ttu-id="0f339-108">Реализация алгоритма нейронной сети (Майкрософт)</span><span class="sxs-lookup"><span data-stu-id="0f339-108">Implementation of the Microsoft Neural Network Algorithm</span></span>  
 <span data-ttu-id="0f339-109">В нейронной сети в виде многослойного перцептрона каждый нейрон получает один или несколько входов, а также создает один или несколько одинаковых выходов.</span><span class="sxs-lookup"><span data-stu-id="0f339-109">In a Multilayer Perceptron neural network, each neuron receives one or more inputs and produces one or more identical outputs.</span></span> <span data-ttu-id="0f339-110">Каждый выход является простой нелинейной функцией суммы входов, полученных нейроном.</span><span class="sxs-lookup"><span data-stu-id="0f339-110">Each output is a simple non-linear function of the sum of the inputs to the neuron.</span></span> <span data-ttu-id="0f339-111">Входы передаются в прямом направлении от узлов во входном слое к узлам в скрытом слое, а оттуда передаются на выходной слой; нейроны в составе слоя не соединены друг с другом.</span><span class="sxs-lookup"><span data-stu-id="0f339-111">Inputs pass forward from nodes in the input layer to nodes in the hidden layer, and then pass from the hidden layer to the output layer; there are no connections between neurons within a layer.</span></span> <span data-ttu-id="0f339-112">Если скрытый слой отсутствует, как в модели логистической регрессии, то входы передаются в прямом направлении непосредственно от узлов входного слоя к узлам выходного слоя.</span><span class="sxs-lookup"><span data-stu-id="0f339-112">If no hidden layer is included, as in a logistic regression model, inputs pass forward directly from nodes in the input layer to nodes in the output layer.</span></span>  
  
 <span data-ttu-id="0f339-113">В нейронной сети, создаваемой с помощью алгоритма нейронной сети ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] ), существует три типа нейронов.</span><span class="sxs-lookup"><span data-stu-id="0f339-113">There are three types of neurons in a neural network that is created with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm:</span></span>  
  
-   `Input neurons`  
  
 <span data-ttu-id="0f339-114">Входные нейроны предоставляют модели интеллектуального анализа данных значения входных атрибутов.</span><span class="sxs-lookup"><span data-stu-id="0f339-114">Input neurons provide input attribute values for the data mining model.</span></span> <span data-ttu-id="0f339-115">Для дискретных входных атрибутов входной нейрон обычно представляет одно состояние из входного атрибута.</span><span class="sxs-lookup"><span data-stu-id="0f339-115">For discrete input attributes, an input neuron typically represents a single state from the input attribute.</span></span> <span data-ttu-id="0f339-116">Сюда входят также отсутствующие значения, если обучающие данные содержат для этого атрибута значения NULL.</span><span class="sxs-lookup"><span data-stu-id="0f339-116">This includes missing values, if the training data contains nulls for that attribute.</span></span> <span data-ttu-id="0f339-117">Дискретный входной атрибут, имеющий более двух состояний, создает один входной нейрон для каждого состояния и один входной нейрон для отсутствующего состояния, если обучающие данные содержат какие-либо значения NULL.</span><span class="sxs-lookup"><span data-stu-id="0f339-117">A discrete input attribute that has more than two states generates one input neuron for each state, and one input neuron for a missing state, if there are any nulls in the training data.</span></span> <span data-ttu-id="0f339-118">Непрерывный входной атрибут создает два входных нейрона: один нейрон для отсутствующего состояния и один нейрон для значения самого непрерывного атрибута.</span><span class="sxs-lookup"><span data-stu-id="0f339-118">A continuous input attribute generates two input neurons: one neuron for a missing state, and one neuron for the value of the continuous attribute itself.</span></span> <span data-ttu-id="0f339-119">Входные нейроны обеспечивают входы для одного или нескольких скрытых нейронов.</span><span class="sxs-lookup"><span data-stu-id="0f339-119">Input neurons provide inputs to one or more hidden neurons.</span></span>  
  
-   `Hidden neurons`  
  
 <span data-ttu-id="0f339-120">Скрытые нейроны получают входные данные от входных нейронов и передают выходные данные выходным нейронам.</span><span class="sxs-lookup"><span data-stu-id="0f339-120">Hidden neurons receive inputs from input neurons and provide outputs to output neurons.</span></span>  
  
-   `Output neurons`  
  
 <span data-ttu-id="0f339-121">Выходные нейроны представляют значения прогнозируемых атрибутов для модели интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="0f339-121">Output neurons represent predictable attribute values for the data mining model.</span></span> <span data-ttu-id="0f339-122">Для дискретных входных атрибутов выходной нейрон обычно указывает одно прогнозируемое состояние для прогнозируемого атрибута, включая пропущенные значения.</span><span class="sxs-lookup"><span data-stu-id="0f339-122">For discrete input attributes, an output neuron typically represents a single predicted state for a predictable attribute, including missing values.</span></span> <span data-ttu-id="0f339-123">Например, бинарный прогнозируемый атрибут создает один выходной узел, который описывает отсутствующее или существующее состояние, указывая на наличие значения для такого атрибута.</span><span class="sxs-lookup"><span data-stu-id="0f339-123">For example, a binary predictable attribute produces one output node that describes a missing or existing state, to indicate whether a value exists for that attribute.</span></span> <span data-ttu-id="0f339-124">Логический столбец, используемый как прогнозируемый, создает три выходных нейрона: один нейрон для истинного значения, один для ложного и один для отсутствующего или существующего состояния.</span><span class="sxs-lookup"><span data-stu-id="0f339-124">A Boolean column that is used as a predictable attribute generates three output neurons: one neuron for a true value, one neuron for a false value, and one neuron for a missing or existing state.</span></span> <span data-ttu-id="0f339-125">Дискретный прогнозируемый атрибут, имеющий более двух состояний, создает один выходной нейрон для каждого состояния и один выходной нейрон для отсутствующего или существующего состояния.</span><span class="sxs-lookup"><span data-stu-id="0f339-125">A discrete predictable attribute that has more than two states generates one output neuron for each state, and one output neuron for a missing or existing state.</span></span> <span data-ttu-id="0f339-126">Непрерывные прогнозируемые столбцы создают два выходных нейрона: один нейрон для отсутствующего или существующего состояния и один нейрон для значения самого непрерывного столбца.</span><span class="sxs-lookup"><span data-stu-id="0f339-126">Continuous predictable columns generate two output neurons: one neuron for a missing or existing state, and one neuron for the value of the continuous column itself.</span></span> <span data-ttu-id="0f339-127">При создании путем просмотра набора прогнозируемых столбцов более 500 выходных нейронов службы [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] формируют для представления дополнительных выходных нейронов новую сеть в модели интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="0f339-127">If more than 500 output neurons are generated by reviewing the set of predictable columns, [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] generates a new network in the mining model to represent the additional output neurons.</span></span>  
  
 <span data-ttu-id="0f339-128">Нейрон получает входы от других нейронов или из других данных, в зависимости от того, в каком слое сети он находится.</span><span class="sxs-lookup"><span data-stu-id="0f339-128">A neuron receives input from other neurons, or from other data, depending on which layer of the network it is in.</span></span> <span data-ttu-id="0f339-129">Входной нейрон получает на входе оригинальные данные.</span><span class="sxs-lookup"><span data-stu-id="0f339-129">An input neuron receives inputs from the original data.</span></span> <span data-ttu-id="0f339-130">Скрытые нейроны и выходные нейроны получают входы из выхода других нейронов нейронной сети.</span><span class="sxs-lookup"><span data-stu-id="0f339-130">Hidden neurons and output neurons receive inputs from the output of other neurons in the neural network.</span></span> <span data-ttu-id="0f339-131">Входы устанавливают связи между нейронами, и эти связи являются путем, по которому производится анализ для конкретного набора вариантов.</span><span class="sxs-lookup"><span data-stu-id="0f339-131">Inputs establish relationships between neurons, and the relationships serve as a path of analysis for a specific set of cases.</span></span>  
  
 <span data-ttu-id="0f339-132">Каждому входу присвоено значение, именуемое *весом*, которое описывает релевантность или важность конкретного входа для скрытого или выходного нейрона.</span><span class="sxs-lookup"><span data-stu-id="0f339-132">Each input has a value assigned to it, called the *weight*, which describes the relevance or importance of that particular input to the hidden neuron or the output neuron.</span></span> <span data-ttu-id="0f339-133">Чем больше вес, присвоенный входу, тем более релевантным или важным является значение этого входа.</span><span class="sxs-lookup"><span data-stu-id="0f339-133">The greater the weight that is assigned to an input, the more relevant or important the value of that input.</span></span> <span data-ttu-id="0f339-134">Значения веса могут быть отрицательными; это означает, что вход может подавлять, а не активировать конкретный нейрон.</span><span class="sxs-lookup"><span data-stu-id="0f339-134">Weights can be negative, which implies that the input can inhibit, rather than activate, a specific neuron.</span></span> <span data-ttu-id="0f339-135">Чтобы выделить важность входа для конкретного нейрона, значение входа умножается на вес.</span><span class="sxs-lookup"><span data-stu-id="0f339-135">The value of each input is multiplied by the weight to emphasize the importance of an input for a specific neuron.</span></span> <span data-ttu-id="0f339-136">В случае отрицательных весов умножение значения на вес служит для уменьшения важности входа.</span><span class="sxs-lookup"><span data-stu-id="0f339-136">For negative weights, the effect of multiplying the value by the weight is to deemphasize the importance.</span></span>  
  
 <span data-ttu-id="0f339-137">Каждому нейрону присвоена простая нелинейная функция, называемая *функцией активации*, которая описывает релевантность или важность определенного нейрона для этого слоя нейронной сети.</span><span class="sxs-lookup"><span data-stu-id="0f339-137">Each neuron has a simple non-linear function assigned to it, called the *activation function*, which describes the relevance or importance of a particular neuron to that layer of a neural network.</span></span> <span data-ttu-id="0f339-138">В качестве функции активации скрытые нейроны используют функцию *гиперболического тангенса* (tanh), а выходные нейроны — *сигмоидальную функцию* .</span><span class="sxs-lookup"><span data-stu-id="0f339-138">Hidden neurons use a *hyperbolic tangent* function (tanh) for their activation function, whereas output neurons use a *sigmoid* function for activation.</span></span> <span data-ttu-id="0f339-139">Обе функции являются нелинейными, непрерывными функциями, позволяющими нейронной сети моделировать нелинейные связи между входными и выходными нейронами.</span><span class="sxs-lookup"><span data-stu-id="0f339-139">Both functions are nonlinear, continuous functions that allow the neural network to model nonlinear relationships between input and output neurons.</span></span>  
  
### <a name="training-neural-networks"></a><span data-ttu-id="0f339-140">Обучение нейронных сетей</span><span class="sxs-lookup"><span data-stu-id="0f339-140">Training Neural Networks</span></span>  
 <span data-ttu-id="0f339-141">Обучение модели интеллектуального анализа данных, использующей алгоритм нейронной сети ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] ), основано на нескольких шагах.</span><span class="sxs-lookup"><span data-stu-id="0f339-141">Several steps are involved in training a data mining model that uses the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="0f339-142">На указанные шаги сильное влияние оказывают заданные значения параметров алгоритма.</span><span class="sxs-lookup"><span data-stu-id="0f339-142">These steps are heavily influenced by the values that you specify for the algorithm parameters.</span></span>  
  
 <span data-ttu-id="0f339-143">Алгоритм сначала оценивает и извлекает обучающие данные из источника данных.</span><span class="sxs-lookup"><span data-stu-id="0f339-143">The algorithm first evaluates and extracts training data from the data source.</span></span> <span data-ttu-id="0f339-144">Определенный процент обучающих данных, называемых *контрольными данными*, зарезервирован для использования при определении точности сети.</span><span class="sxs-lookup"><span data-stu-id="0f339-144">A percentage of the training data, called the *holdout data*, is reserved for use in assessing the accuracy of the network.</span></span> <span data-ttu-id="0f339-145">В процессе обучения после каждой итерации по обучающим данным выполняется оценка сети.</span><span class="sxs-lookup"><span data-stu-id="0f339-145">Throughout the training process, the network is evaluated immediately after each iteration through the training data.</span></span> <span data-ttu-id="0f339-146">После прекращения роста точности модели процесс обучения завершается.</span><span class="sxs-lookup"><span data-stu-id="0f339-146">When the accuracy no longer increases, the training process is stopped.</span></span>  
  
 <span data-ttu-id="0f339-147">Значения параметров *SAMPLE_SIZE* и *HOLDOUT_PERCENTAGE* используются для определения количества вариантов, которое необходимо выбрать из обучающих данных, а также количества вариантов, которое необходимо зарезервировать для использования в качестве контрольных данных.</span><span class="sxs-lookup"><span data-stu-id="0f339-147">The values of the *SAMPLE_SIZE* and *HOLDOUT_PERCENTAGE* parameters are used to determine the number of cases to sample from the training data and the number of cases to be put aside for the holdout data.</span></span> <span data-ttu-id="0f339-148">Значение параметра *HOLDOUT_SEED* используется для определения случайным образом конкретных вариантов, подлежащих резервированию в качестве контрольных данных.</span><span class="sxs-lookup"><span data-stu-id="0f339-148">The value of the *HOLDOUT_SEED* parameter is used to randomly determine the individual cases to be put aside for the holdout data.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="0f339-149">Эти параметры алгоритма отличны от свойств HOLDOUT_SIZE и HOLDOUT_SEED, применяемых к структуре интеллектуального анализа данных для определения набора проверочных данных.</span><span class="sxs-lookup"><span data-stu-id="0f339-149">These algorithm parameters are different from the HOLDOUT_SIZE and HOLDOUT_SEED properties, which are applied to a mining structure to define a testing data set.</span></span>  
  
 <span data-ttu-id="0f339-150">Затем алгоритм определяет количество и сложность сетей, поддерживаемых моделью интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="0f339-150">The algorithm next determines the number and complexity of the networks that the mining model supports.</span></span> <span data-ttu-id="0f339-151">Если в модели интеллектуального анализа данных содержится один или несколько атрибутов, используемых только для прогнозирования, то алгоритм создает одну сеть, которая представляет все эти атрибуты.</span><span class="sxs-lookup"><span data-stu-id="0f339-151">If the mining model contains one or more attributes that are used only for prediction, the algorithm creates a single network that represents all such attributes.</span></span> <span data-ttu-id="0f339-152">Если в модели интеллектуального анализа данных содержится один или несколько атрибутов, используемых как для ввода, так и для прогнозирования, то поставщик алгоритмов создает сеть для каждого такого атрибута.</span><span class="sxs-lookup"><span data-stu-id="0f339-152">If the mining model contains one or more attributes that are used for both input and prediction, the algorithm provider constructs a network for each attribute.</span></span>  
  
 <span data-ttu-id="0f339-153">Для входных и прогнозируемых атрибутов, которые имеют дискретные значения, каждый входной или выходной нейрон соответственно представляет одно состояние.</span><span class="sxs-lookup"><span data-stu-id="0f339-153">For input and predictable attributes that have discrete values, each input or output neuron respectively represents a single state.</span></span> <span data-ttu-id="0f339-154">Для входных и прогнозируемых атрибутов, которые имеют непрерывные атрибуты, каждый входной или выходной нейрон соответственно представляет диапазон и распределение значений атрибута.</span><span class="sxs-lookup"><span data-stu-id="0f339-154">For input and predictable attributes that have continuous values, each input or output neuron respectively represents the range and distribution of values for the attribute.</span></span> <span data-ttu-id="0f339-155">И в том, и в другом случае максимальное количество поддерживаемых состояний зависит от значения параметра алгоритма *MAXIMUM_STATES* .</span><span class="sxs-lookup"><span data-stu-id="0f339-155">The maximum number of states that is supported in either case depends on the value of the *MAXIMUM_STATES* algorithm parameter.</span></span> <span data-ttu-id="0f339-156">Если количество состояний конкретного атрибута превышает значение параметра алгоритма *MAXIMUM_STATES* , то выбираются наиболее популярные или релевантные состояния такого атрибута в количестве, равном максимально возможному количеству состояний. Для целей анализа остальные состояния группируются как пропущенные значения.</span><span class="sxs-lookup"><span data-stu-id="0f339-156">If the number of states for a specific attribute exceeds the value of the *MAXIMUM_STATES* algorithm parameter, the most popular or relevant states for that attribute are chosen, up to the maximum number of states allowed, and the remaining states are grouped as missing values for the purposes of analysis.</span></span>  
  
 <span data-ttu-id="0f339-157">Алгоритм затем использует значение параметра *HIDDEN_NODE_RATIO* при определении начального количества нейронов, необходимых для создания скрытого слоя.</span><span class="sxs-lookup"><span data-stu-id="0f339-157">The algorithm then uses the value of the *HIDDEN_NODE_RATIO* parameter when determining the initial number of neurons to create for the hidden layer.</span></span> <span data-ttu-id="0f339-158">Для интерпретации нейронной сети как логистической регрессии можно задать параметр *HIDDEN_NODE_RATIO* равным 0, чтобы предотвратить в сетях создание скрытого слоя, создаваемого алгоритмом для модели интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="0f339-158">You can set *HIDDEN_NODE_RATIO* to 0 to prevent the creation of a hidden layer in the networks that the algorithm generates for the mining model, to treat the neural network as a logistic regression.</span></span>  
  
 <span data-ttu-id="0f339-159">Поставщик алгоритмов циклично оценивает вес всех одновременно появляющихся в сети входов. Для этого берется набор предварительно зарезервированных обучающих данных и сравнивается фактическое известное значение каждого объекта в составе контрольных данных с прогнозом сети, — этот процесс известен как *пакетное обучение*.</span><span class="sxs-lookup"><span data-stu-id="0f339-159">The algorithm provider iteratively evaluates the weight for all inputs across the network at the same time, by taking the set of training data that was reserved earlier and comparing the actual known value for each case in the holdout data with the network's prediction, in a process known as *batch learning*.</span></span> <span data-ttu-id="0f339-160">После оценки всего набора обучающих данных алгоритм просматривает прогнозированное и фактическое значение каждого нейрона.</span><span class="sxs-lookup"><span data-stu-id="0f339-160">After the algorithm has evaluated the entire set of training data, the algorithm reviews the predicted and actual value for each neuron.</span></span> <span data-ttu-id="0f339-161">Алгоритм вычисляет погрешность, если таковая имеется, и регулирует весовые значения, связанные с входами для данного нейрона, обрабатывая сначала выходные, а затем входные нейроны, — этот процесс известен как *обратное распространение*.</span><span class="sxs-lookup"><span data-stu-id="0f339-161">The algorithm calculates the degree of error, if any, and adjusts the weights that are associated with the inputs for that neuron, working backward from output neurons to input neurons in a process known as *backpropagation*.</span></span> <span data-ttu-id="0f339-162">Алгоритм затем повторяет процесс в отношении всего набора обучающих данных.</span><span class="sxs-lookup"><span data-stu-id="0f339-162">The algorithm then repeats the process over the entire set of training data.</span></span> <span data-ttu-id="0f339-163">Поскольку алгоритм может поддерживать множество весовых значений и выходных нейронов, то для управления процессом обучения, присваивающим и оценивающим весовые значения входов, используется алгоритм сопряженных градиентов.</span><span class="sxs-lookup"><span data-stu-id="0f339-163">Because the algorithm can support many weights and output neurons, the conjugate gradient algorithm is used to guide the training process for assigning and evaluating weights for inputs.</span></span> <span data-ttu-id="0f339-164">В задачи настоящей документации не входит рассмотрение алгоритма сопряженных градиентов.</span><span class="sxs-lookup"><span data-stu-id="0f339-164">A discussion of the conjugate gradient algorithm is outside the scope of this documentation.</span></span>  
  
### <a name="feature-selection"></a><span data-ttu-id="0f339-165">Выбор компонентов</span><span class="sxs-lookup"><span data-stu-id="0f339-165">Feature Selection</span></span>  
 <span data-ttu-id="0f339-166">Если количество входных или прогнозируемых атрибутов превышает соответственно значение параметра *MAXIMUM_INPUT_ATTRIBUTES* или *MAXIMUM_OUTPUT_ATTRIBUTES* , то для снижения сложности сетей, включенных в модель интеллектуального анализа данных, используется алгоритм выбора компонентов.</span><span class="sxs-lookup"><span data-stu-id="0f339-166">If the number of input attributes is greater than the value of the *MAXIMUM_INPUT_ATTRIBUTES* parameter, or if the number of predictable attributes is greater than the value of the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, a feature selection algorithm is used to reduce the complexity of the networks that are included in the mining model.</span></span> <span data-ttu-id="0f339-167">Выбор компонентов уменьшает количество входных или прогнозируемых атрибутов, ограничивая их теми атрибутами, которые с точки зрения статистики наиболее релевантны для такой модели.</span><span class="sxs-lookup"><span data-stu-id="0f339-167">Feature selection reduces the number of input or predictable attributes to those that are most statistically relevant to the model.</span></span>  
  
 <span data-ttu-id="0f339-168">Выбор компонентов автоматически применяется всеми алгоритмами интеллектуального анализа данных служб [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] для улучшения качества анализа и снижения вычислительной нагрузки.</span><span class="sxs-lookup"><span data-stu-id="0f339-168">Feature selection is used automatically by all [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] data mining algorithms to improve analysis and reduce processing load.</span></span> <span data-ttu-id="0f339-169">Применяемый метод выбора компонентов в моделях нейронных сетей зависит от типа данных атрибута.</span><span class="sxs-lookup"><span data-stu-id="0f339-169">The method used for feature selection in neural network models depends on the data type of the attribute.</span></span> <span data-ttu-id="0f339-170">Для справки, следующая таблица показывает методы выбора компонентов, применяемые в моделях нейронных сетей, а также методы выбора компонентов, используемые для алгоритма логистической регрессии, основанного на алгоритме нейронной сети.</span><span class="sxs-lookup"><span data-stu-id="0f339-170">For reference, the following table shows the feature selection methods used for neural network models, and also shows the feature selection methods used for the Logistic Regression algorithm, which is based on the Neural Network algorithm.</span></span>  
  
|<span data-ttu-id="0f339-171">Алгоритм</span><span class="sxs-lookup"><span data-stu-id="0f339-171">Algorithm</span></span>|<span data-ttu-id="0f339-172">Метод анализа</span><span class="sxs-lookup"><span data-stu-id="0f339-172">Method of analysis</span></span>|<span data-ttu-id="0f339-173">Комментарии</span><span class="sxs-lookup"><span data-stu-id="0f339-173">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="0f339-174">Нейронная сеть</span><span class="sxs-lookup"><span data-stu-id="0f339-174">Neural Network</span></span>|<span data-ttu-id="0f339-175">Оценка интересности</span><span class="sxs-lookup"><span data-stu-id="0f339-175">Interestingness score</span></span><br /><br /> <span data-ttu-id="0f339-176">Энтропия Шеннона</span><span class="sxs-lookup"><span data-stu-id="0f339-176">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="0f339-177">Алгоритм Байеса с априорной оценкой K2</span><span class="sxs-lookup"><span data-stu-id="0f339-177">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="0f339-178">Эквивалент Дирихле метода Байеса с однородной априорной оценкой (выбор по умолчанию)</span><span class="sxs-lookup"><span data-stu-id="0f339-178">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="0f339-179">В алгоритме нейронных сетей могут применяться оба метода: на основе энтропии и байесовских оценок — при условии, что данные содержат непрерывные столбцы.</span><span class="sxs-lookup"><span data-stu-id="0f339-179">The Neural Networks algorithm can use both entropy-based and Bayesian scoring methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="0f339-180">По умолчанию.</span><span class="sxs-lookup"><span data-stu-id="0f339-180">Default.</span></span>|  
|<span data-ttu-id="0f339-181">Логистическая регрессия</span><span class="sxs-lookup"><span data-stu-id="0f339-181">Logistic Regression</span></span>|<span data-ttu-id="0f339-182">Оценка интересности</span><span class="sxs-lookup"><span data-stu-id="0f339-182">Interestingness score</span></span><br /><br /> <span data-ttu-id="0f339-183">Энтропия Шеннона</span><span class="sxs-lookup"><span data-stu-id="0f339-183">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="0f339-184">Алгоритм Байеса с априорной оценкой K2</span><span class="sxs-lookup"><span data-stu-id="0f339-184">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="0f339-185">Эквивалент Дирихле метода Байеса с однородной априорной оценкой (выбор по умолчанию)</span><span class="sxs-lookup"><span data-stu-id="0f339-185">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="0f339-186">Возможность передать параметр в этот алгоритм для управления поведением при выборе характеристик отсутствует, поэтому используются значения по умолчанию.</span><span class="sxs-lookup"><span data-stu-id="0f339-186">Because you cannot pass a parameter to this algorithm to control feature election behavior, the defaults are used.</span></span> <span data-ttu-id="0f339-187">Таким образом, если все атрибуты являются дискретными или дискретизированными, то по умолчанию используется метод BDEU.</span><span class="sxs-lookup"><span data-stu-id="0f339-187">Therefore, if all attributes are discrete or discretized, the default is BDEU.</span></span>|  
  
 <span data-ttu-id="0f339-188">Выбором компонентов в модели нейронной сети управляют следующие параметры алгоритма: MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES и MAXIMUM_STATES.</span><span class="sxs-lookup"><span data-stu-id="0f339-188">The algorithm parameters that control feature selection for a neural network model are MAXIMUM_INPUT_ATTRIBUTES, MAXIMUM_OUTPUT_ATTRIBUTES, and MAXIMUM_STATES.</span></span> <span data-ttu-id="0f339-189">Число скрытых слоев можно также регулировать с помощью параметра HIDDEN_NODE_RATIO.</span><span class="sxs-lookup"><span data-stu-id="0f339-189">You can also control the number of hidden layers by setting the HIDDEN_NODE_RATIO parameter.</span></span>  
  
### <a name="scoring-methods"></a><span data-ttu-id="0f339-190">Методы количественной оценки</span><span class="sxs-lookup"><span data-stu-id="0f339-190">Scoring Methods</span></span>  
 <span data-ttu-id="0f339-191">*Количественная оценка* представляет собой разновидность нормализации, и в контексте модели нейронной сети означает процесс преобразования величины (например дискретной текстовой метки) в значение, которое можно сравнить с другими типами входов и присвоить ему определенный вес в сети.</span><span class="sxs-lookup"><span data-stu-id="0f339-191">*Scoring* is a kind of normalization, which in the context of training a neural network model means the process of converting a value, such as a discrete text label, into a value that can be compared with other types of inputs and weighted in the network.</span></span> <span data-ttu-id="0f339-192">Например, если один входной атрибут — «Пол», с возможными значениями «Мужской» и «Женский», а другой атрибут — «Доход» с переменным диапазоном значений, величины этих двух атрибутов нельзя напрямую сравнивать между собой; их нужно закодировать на некоторой общей шкале, чтобы иметь возможность сравнить их веса.</span><span class="sxs-lookup"><span data-stu-id="0f339-192">For example, if one input attribute is Gender and the possible values are Male and Female, and another input attribute is Income, with a variable range of values, the values for each attribute are not directly comparable, and therefore must be encoded to a common scale so that the weights can be computed.</span></span> <span data-ttu-id="0f339-193">Количественной оценкой называется процесс нормализации таких входов для превращения их в числовые значения, а именно в вероятностный диапазон.</span><span class="sxs-lookup"><span data-stu-id="0f339-193">Scoring is the process of normalizing such inputs to numeric values: specifically, to a probability range.</span></span> <span data-ttu-id="0f339-194">Функции, которые используются для нормализации, помогают также более равномерно распределять входные значения на однородной шкале, чтобы экстремальные значения не искажали результатов анализа.</span><span class="sxs-lookup"><span data-stu-id="0f339-194">The functions used for normalization also help to distribute input value more evenly on a uniform scale so that extreme values do not distort the results of analysis.</span></span>  
  
 <span data-ttu-id="0f339-195">Выходы нейронной сети также закодированы.</span><span class="sxs-lookup"><span data-stu-id="0f339-195">Outputs of the neural network are also encoded.</span></span> <span data-ttu-id="0f339-196">Если у выхода только одна цель (то есть прогноз) или несколько целей, которые используются только для прогноза, но не для входа, модель создает одну сеть, в которой нормализация, возможно, не потребуется.</span><span class="sxs-lookup"><span data-stu-id="0f339-196">When there is a single target for output (that is, prediction), or multiple targets that are used for prediction only and not for input, the model create a single network and it might not seem necessary to normalize the values.</span></span> <span data-ttu-id="0f339-197">Однако если несколько атрибутов используются для ввода и прогнозирования, модель создает несколько сетей; тогда все величины должны быть нормализованы и выходы закодированы в момент выхода из сети.</span><span class="sxs-lookup"><span data-stu-id="0f339-197">However, if multiple attributes are used for input and prediction, the model must create multiple networks; therefore, all values must be normalized, and the outputs too must be encoded as they exit the network.</span></span>  
  
 <span data-ttu-id="0f339-198">Кодирование входов производится суммированием всех дискретных величин в обучающих вариантах и умножением этой величины на ее вес.</span><span class="sxs-lookup"><span data-stu-id="0f339-198">Encoding for inputs is based on summing each discrete value in the training cases, and multiplying that value by its weight.</span></span> <span data-ttu-id="0f339-199">Это называется *взвешенной суммой*. Она передается функции активации в скрытом слое.</span><span class="sxs-lookup"><span data-stu-id="0f339-199">This is called a *weighted sum*, which is passed to the activation function in the hidden layer.</span></span> <span data-ttu-id="0f339-200">Для кодирования используется z-показатель, как показано ниже.</span><span class="sxs-lookup"><span data-stu-id="0f339-200">A z-score is used for encoding, as follows:</span></span>  
  
 <span data-ttu-id="0f339-201">**Дискретные значения**</span><span class="sxs-lookup"><span data-stu-id="0f339-201">**Discrete values**</span></span>  
  
 <span data-ttu-id="0f339-202">μ = p — прошлая вероятность состояния</span><span class="sxs-lookup"><span data-stu-id="0f339-202">μ = p - the prior probability of a state</span></span>  
  
 <span data-ttu-id="0f339-203">StdDev  = sqrt(p(1-p))</span><span class="sxs-lookup"><span data-stu-id="0f339-203">StdDev  = sqrt(p(1-p))</span></span>  
  
 <span data-ttu-id="0f339-204">**Непрерывные величины**</span><span class="sxs-lookup"><span data-stu-id="0f339-204">**Continuous values**</span></span>  
  
 <span data-ttu-id="0f339-205">Значение имеется = 1-μ/σ</span><span class="sxs-lookup"><span data-stu-id="0f339-205">Value present= 1 - μ/σ</span></span>  
  
 <span data-ttu-id="0f339-206">Нет существующего значения =-μ/σ</span><span class="sxs-lookup"><span data-stu-id="0f339-206">No existing value= -μ/σ</span></span>  
  
 <span data-ttu-id="0f339-207">После кодирования всех величин производится взвешенное суммирование входов, в котором в качестве весов используются интенсивности ребер сети.</span><span class="sxs-lookup"><span data-stu-id="0f339-207">After the values have been encoded, the inputs go through weighted summing, with network edges as weights.</span></span>  
  
 <span data-ttu-id="0f339-208">Кодирование выходов производится с помощью сигмоидальной функции, которую очень удобно использовать для прогнозов.</span><span class="sxs-lookup"><span data-stu-id="0f339-208">Encoding for outputs uses the sigmoid function, which has properties that make it very useful for prediction.</span></span> <span data-ttu-id="0f339-209">Одно из полезных свойств этой функции заключается в том, что, независимо от масштабирования исходных данных и от того, положительны или отрицательны значения, результат этой функции всегда является числовым значением в диапазоне от 0 до 1. Это полезно при оценке вероятностей.</span><span class="sxs-lookup"><span data-stu-id="0f339-209">One such property is that, regardless of how the original values are scaled, and regardless of whether values are negative or positive, the output of this function is always a value between 0 and 1, which is suited for estimating probabilities.</span></span> <span data-ttu-id="0f339-210">Другое полезное свойство сигмоидальной функции — ее сглаживающий эффект: по мере того, как величины удаляются от точки перегиба, вероятность данного значения движется от 0 к 1, но плавно.</span><span class="sxs-lookup"><span data-stu-id="0f339-210">Another useful property is that the sigmoid function has a smoothing effect, so that as values move farther away from point of inflection, the probability for the value moves towards 0 or 1, but slowly.</span></span>  
  
## <a name="customizing-the-neural-network-algorithm"></a><span data-ttu-id="0f339-211">Настройка алгоритма нейронной сети (Майкрософт)</span><span class="sxs-lookup"><span data-stu-id="0f339-211">Customizing the Neural Network Algorithm</span></span>  
 <span data-ttu-id="0f339-212">Алгоритм нейронной сети ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] ) поддерживает несколько параметров, которые влияют на поведение, производительность и точность итоговой модели интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="0f339-212">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports several parameters that affect the behavior, performance, and accuracy of the resulting mining model.</span></span> <span data-ttu-id="0f339-213">Можно также изменять способ обработки данных в модели, устанавливая на столбцах флаги моделирования или устанавливая флаги распределения, чтобы задать способы обработки значений столбцов.</span><span class="sxs-lookup"><span data-stu-id="0f339-213">You can also modify the way that the model processes data by setting modeling flags on columns, or by setting distribution flags to specify how values within the column are handled.</span></span>  
  
### <a name="setting-algorithm-parameters"></a><span data-ttu-id="0f339-214">Задание параметров алгоритма</span><span class="sxs-lookup"><span data-stu-id="0f339-214">Setting Algorithm Parameters</span></span>  
 <span data-ttu-id="0f339-215">В следующей таблице описываются параметры, которые можно использовать с алгоритмом нейронной сети (Майкрософт).</span><span class="sxs-lookup"><span data-stu-id="0f339-215">The following table describes the parameters that can be used with the Microsoft Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="0f339-216">HIDDEN_NODE_RATIO</span><span class="sxs-lookup"><span data-stu-id="0f339-216">HIDDEN_NODE_RATIO</span></span>  
 <span data-ttu-id="0f339-217">Указывает соотношение скрытых нейронов к входным и выходным нейронам.</span><span class="sxs-lookup"><span data-stu-id="0f339-217">Specifies the ratio of hidden neurons to input and output neurons.</span></span> <span data-ttu-id="0f339-218">Следующая формула определяет начальное количество нейронов в скрытом слое:</span><span class="sxs-lookup"><span data-stu-id="0f339-218">The following formula determines the initial number of neurons in the hidden layer:</span></span>  
  
 <span data-ttu-id="0f339-219">HIDDEN_NODE_RATIO \* SQRT (количество входных нейронов \* количество выходных нейронов)</span><span class="sxs-lookup"><span data-stu-id="0f339-219">HIDDEN_NODE_RATIO \* SQRT(Total input neurons \* Total output neurons)</span></span>  
  
 <span data-ttu-id="0f339-220">Значение по умолчанию — 4,0.</span><span class="sxs-lookup"><span data-stu-id="0f339-220">The default value is 4.0.</span></span>  
  
 <span data-ttu-id="0f339-221">HOLDOUT_PERCENTAGE</span><span class="sxs-lookup"><span data-stu-id="0f339-221">HOLDOUT_PERCENTAGE</span></span>  
 <span data-ttu-id="0f339-222">Указывает процент вариантов в составе обучающих данных, используемых для вычисления ошибки контрольных данных, которая применяется как один из критериев остановки во время обучения модели интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="0f339-222">Specifies the percentage of cases within the training data used to calculate the holdout error, which is used as part of the stopping criteria while training the mining model.</span></span>  
  
 <span data-ttu-id="0f339-223">Значение по умолчанию — 30.</span><span class="sxs-lookup"><span data-stu-id="0f339-223">The default value is 30.</span></span>  
  
 <span data-ttu-id="0f339-224">HOLDOUT_SEED</span><span class="sxs-lookup"><span data-stu-id="0f339-224">HOLDOUT_SEED</span></span>  
 <span data-ttu-id="0f339-225">Указывает значение, используемое генератором псевдослучайных чисел в качестве начального, когда алгоритм случайным образом задает контрольные данные.</span><span class="sxs-lookup"><span data-stu-id="0f339-225">Specifies a number that is used to seed the pseudo-random generator when the algorithm randomly determines the holdout data.</span></span> <span data-ttu-id="0f339-226">При установке данного параметра равным 0 алгоритм формирует начальное значение на основе имени модели интеллектуального анализа данных, что гарантирует неизменность содержимого модели при повторной обработке.</span><span class="sxs-lookup"><span data-stu-id="0f339-226">If this parameter is set to 0, the algorithm generates the seed based on the name of the mining model, to guarantee that the model content remains the same during reprocessing.</span></span>  
  
 <span data-ttu-id="0f339-227">Значение по умолчанию — 0.</span><span class="sxs-lookup"><span data-stu-id="0f339-227">The default value is 0.</span></span>  
  
 <span data-ttu-id="0f339-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="0f339-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="0f339-229">Определяет максимальное количество входных атрибутов, которое может быть задано для алгоритма до использования выбора компонентов.</span><span class="sxs-lookup"><span data-stu-id="0f339-229">Determines the maximum number of input attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="0f339-230">Установка этого значения равным 0 отключает выбор компонентов для входных атрибутов.</span><span class="sxs-lookup"><span data-stu-id="0f339-230">Setting this value to 0 disables feature selection for input attributes.</span></span>  
  
 <span data-ttu-id="0f339-231">Значение по умолчанию — 255.</span><span class="sxs-lookup"><span data-stu-id="0f339-231">The default value is 255.</span></span>  
  
 <span data-ttu-id="0f339-232">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="0f339-232">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="0f339-233">Определяет максимальное количество выходных атрибутов, которое может быть задано для алгоритма до использования выбора компонентов.</span><span class="sxs-lookup"><span data-stu-id="0f339-233">Determines the maximum number of output attributes that can be supplied to the algorithm before feature selection is employed.</span></span> <span data-ttu-id="0f339-234">Установка этого значения равным 0 отключает выбор компонентов для выходных атрибутов.</span><span class="sxs-lookup"><span data-stu-id="0f339-234">Setting this value to 0 disables feature selection for output attributes.</span></span>  
  
 <span data-ttu-id="0f339-235">Значение по умолчанию — 255.</span><span class="sxs-lookup"><span data-stu-id="0f339-235">The default value is 255.</span></span>  
  
 <span data-ttu-id="0f339-236">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="0f339-236">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="0f339-237">Указывает максимальное число дискретных состояний на один атрибут, поддерживаемое алгоритмом.</span><span class="sxs-lookup"><span data-stu-id="0f339-237">Specifies the maximum number of discrete states per attribute that is supported by the algorithm.</span></span> <span data-ttu-id="0f339-238">Если число состояний конкретного атрибута превышает число, указанное для данного параметра, то алгоритм использует наиболее популярные состояния такого атрибута и считает остальные состояния пропущенными.</span><span class="sxs-lookup"><span data-stu-id="0f339-238">If the number of states for a specific attribute is greater than the number that is specified for this parameter, the algorithm uses the most popular states for that attribute and treats the remaining states as missing.</span></span>  
  
 <span data-ttu-id="0f339-239">Значение по умолчанию — 100.</span><span class="sxs-lookup"><span data-stu-id="0f339-239">The default value is 100.</span></span>  
  
 <span data-ttu-id="0f339-240">SAMPLE_SIZE</span><span class="sxs-lookup"><span data-stu-id="0f339-240">SAMPLE_SIZE</span></span>  
 <span data-ttu-id="0f339-241">Указывает количество вариантов, которые будут использоваться для обучения модели.</span><span class="sxs-lookup"><span data-stu-id="0f339-241">Specifies the number of cases to be used to train the model.</span></span> <span data-ttu-id="0f339-242">Алгоритм использует меньшее из двух значений — либо число, либо заданный параметром HOLDOUT_PERCENTAGE процент от общего количества вариантов, не включенных в состав контрольных данных.</span><span class="sxs-lookup"><span data-stu-id="0f339-242">The algorithm uses either this number or the percentage of total of cases not included in the holdout data as specified by the HOLDOUT_PERCENTAGE parameter, whichever value is smaller.</span></span>  
  
 <span data-ttu-id="0f339-243">Другими словами, если для параметра HOLDOUT_PERCENTAGE задано значение 30, то алгоритм будет использовать либо значение этого параметра, либо значение, равное 70 процентам от общего количества вариантов, в зависимости от того, какое из двух указанных значений меньше.</span><span class="sxs-lookup"><span data-stu-id="0f339-243">In other words, if HOLDOUT_PERCENTAGE is set to 30, the algorithm will use either the value of this parameter, or a value equal to 70 percent of the total number of cases, whichever is smaller.</span></span>  
  
 <span data-ttu-id="0f339-244">Значение по умолчанию — 10 000.</span><span class="sxs-lookup"><span data-stu-id="0f339-244">The default value is 10000.</span></span>  
  
### <a name="modeling-flags"></a><span data-ttu-id="0f339-245">Флаги моделирования</span><span class="sxs-lookup"><span data-stu-id="0f339-245">Modeling Flags</span></span>  
 <span data-ttu-id="0f339-246">Далее перечислены флаги модели, которые поддерживает алгоритм нейронной сети ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] ).</span><span class="sxs-lookup"><span data-stu-id="0f339-246">The following modeling flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span>  
  
 <span data-ttu-id="0f339-247">NOT NULL</span><span class="sxs-lookup"><span data-stu-id="0f339-247">NOT NULL</span></span>  
 <span data-ttu-id="0f339-248">Указывает, что столбец не может принимать значение NULL.</span><span class="sxs-lookup"><span data-stu-id="0f339-248">Indicates that the column cannot contain a null.</span></span> <span data-ttu-id="0f339-249">Если во время обучения модели службы Analysis Services обнаружат значение NULL, возникнет ошибка.</span><span class="sxs-lookup"><span data-stu-id="0f339-249">An error will result if Analysis Services encounters a null during model training.</span></span>  
  
 <span data-ttu-id="0f339-250">Применяется к столбцам структуры интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="0f339-250">Applies to mining structure columns.</span></span>  
  
 <span data-ttu-id="0f339-251">MODEL_EXISTENCE_ONLY</span><span class="sxs-lookup"><span data-stu-id="0f339-251">MODEL_EXISTENCE_ONLY</span></span>  
 <span data-ttu-id="0f339-252">Указывает, что модель должна принимать во внимание только существование или отсутствие значения для данного атрибута.</span><span class="sxs-lookup"><span data-stu-id="0f339-252">Indicates that the model should only consider whether a value exists for the attribute or if a value is missing.</span></span> <span data-ttu-id="0f339-253">Конкретное значение не играет роли.</span><span class="sxs-lookup"><span data-stu-id="0f339-253">The exact value does not matter.</span></span>  
  
 <span data-ttu-id="0f339-254">Применяется к столбцам модели интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="0f339-254">Applies to mining model columns.</span></span>  
  
### <a name="distribution-flags"></a><span data-ttu-id="0f339-255">Флаги распределения</span><span class="sxs-lookup"><span data-stu-id="0f339-255">Distribution Flags</span></span>  
 <span data-ttu-id="0f339-256">Далее перечислены флаги распределения, которые поддерживает алгоритм нейронной сети ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] ).</span><span class="sxs-lookup"><span data-stu-id="0f339-256">The following distribution flags are supported for use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm.</span></span> <span data-ttu-id="0f339-257">Модель рассматривает эти флаги только как указания; если алгоритм обнаруживает иное распределение, то использует его, не учитывая указания.</span><span class="sxs-lookup"><span data-stu-id="0f339-257">The flags are used as hints to the model only; if the algorithm detects a different distribution it will use the found distribution, not the distribution provided in the hint.</span></span>  
  
 <span data-ttu-id="0f339-258">Норм.</span><span class="sxs-lookup"><span data-stu-id="0f339-258">Normal</span></span>  
 <span data-ttu-id="0f339-259">Указывает, что величины в столбце должны обрабатываться так, как если бы они представляли собой нормальное, или гауссово, распределение.</span><span class="sxs-lookup"><span data-stu-id="0f339-259">Indicates that values within the column should be treated as though they represent the normal, or Gaussian, distribution.</span></span>  
  
 <span data-ttu-id="0f339-260">Равномерное</span><span class="sxs-lookup"><span data-stu-id="0f339-260">Uniform</span></span>  
 <span data-ttu-id="0f339-261">Указывает, что значения этого столбца следует обрабатывать, как если бы они были распределены равномерно; то есть, как если бы вероятность появления любого значения была бы примерно одинаковой и являлась функцией от общего числа значений.</span><span class="sxs-lookup"><span data-stu-id="0f339-261">Indicates that values within the column should be treated as though they are distributed uniformly; that is, the probability of any value is roughly equal, and is a function of the total number of values.</span></span>  
  
 <span data-ttu-id="0f339-262">Логарифмическое нормальное</span><span class="sxs-lookup"><span data-stu-id="0f339-262">Log Normal</span></span>  
 <span data-ttu-id="0f339-263">Указывает, что значения этого столбца следует обрабатывать, как если бы они были распределены по *логарифмически нормальной* кривой; то есть логарифм этих значений имеет нормальное распределение.</span><span class="sxs-lookup"><span data-stu-id="0f339-263">Indicates that values within the column should be treated as though distributed according to the *log normal* curve, which means that the logarithm of the values is distributed normally.</span></span>  
  
## <a name="requirements"></a><span data-ttu-id="0f339-264">Требования</span><span class="sxs-lookup"><span data-stu-id="0f339-264">Requirements</span></span>  
 <span data-ttu-id="0f339-265">Модель нейронной сети должна содержать по крайней мере один входной столбец и один выходной столбец.</span><span class="sxs-lookup"><span data-stu-id="0f339-265">A neural network model must contain at least one input column and one output column.</span></span>  
  
### <a name="input-and-predictable-columns"></a><span data-ttu-id="0f339-266">Входные и прогнозируемые столбцы</span><span class="sxs-lookup"><span data-stu-id="0f339-266">Input and Predictable Columns</span></span>  
 <span data-ttu-id="0f339-267">Алгоритм нейронной сети ( [!INCLUDE[msCoName](../../includes/msconame-md.md)] ) поддерживает определенные входные столбцы данных и прогнозируемые столбцы, которые перечислены ниже в таблице.</span><span class="sxs-lookup"><span data-stu-id="0f339-267">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Neural Network algorithm supports the specific input columns and predictable columns that are listed in the following table.</span></span>  
  
|<span data-ttu-id="0f339-268">Столбец</span><span class="sxs-lookup"><span data-stu-id="0f339-268">Column</span></span>|<span data-ttu-id="0f339-269">Типы содержимого</span><span class="sxs-lookup"><span data-stu-id="0f339-269">Content types</span></span>|  
|------------|-------------------|  
|<span data-ttu-id="0f339-270">Входной атрибут</span><span class="sxs-lookup"><span data-stu-id="0f339-270">Input attribute</span></span>|<span data-ttu-id="0f339-271">Continuous, Cyclical, Discrete, Discretized, Key, Table и Ordered</span><span class="sxs-lookup"><span data-stu-id="0f339-271">Continuous, Cyclical, Discrete, Discretized, Key, Table, and Ordered</span></span>|  
|<span data-ttu-id="0f339-272">Прогнозируемый атрибут</span><span class="sxs-lookup"><span data-stu-id="0f339-272">Predictable attribute</span></span>|<span data-ttu-id="0f339-273">Continuous, Cyclical, Discrete, Discretized и Ordered</span><span class="sxs-lookup"><span data-stu-id="0f339-273">Continuous, Cyclical, Discrete, Discretized, and Ordered</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="0f339-274">Типы содержимого Cyclical и Ordered поддерживаются, но алгоритм обрабатывает их как дискретные величины и не производит их особой обработки.</span><span class="sxs-lookup"><span data-stu-id="0f339-274">Cyclical and Ordered content types are supported, but the algorithm treats them as discrete values and does not perform special processing.</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="0f339-275">См. также:</span><span class="sxs-lookup"><span data-stu-id="0f339-275">See Also</span></span>  
 <span data-ttu-id="0f339-276">[Алгоритм нейронной сети (Майкрософт)](microsoft-neural-network-algorithm.md) </span><span class="sxs-lookup"><span data-stu-id="0f339-276">[Microsoft Neural Network Algorithm](microsoft-neural-network-algorithm.md) </span></span>  
 <span data-ttu-id="0f339-277">[Содержимое моделей интеллектуального анализа данных для моделей нейронных сетей &#40;Analysis Services — интеллектуальный анализ&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span><span class="sxs-lookup"><span data-stu-id="0f339-277">[Mining Model Content for Neural Network Models &#40;Analysis Services - Data Mining&#41;](mining-model-content-for-neural-network-models-analysis-services-data-mining.md) </span></span>  
 [<span data-ttu-id="0f339-278">Примеры запросов к модели нейронной сети</span><span class="sxs-lookup"><span data-stu-id="0f339-278">Neural Network Model Query Examples</span></span>](neural-network-model-query-examples.md)  
  
  
