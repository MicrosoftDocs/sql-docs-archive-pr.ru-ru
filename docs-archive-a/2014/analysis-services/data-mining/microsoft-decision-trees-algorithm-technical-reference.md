---
title: Технический справочник по алгоритму дерева принятия решений (Майкрософт) | Документация Майкрософт
ms.custom: ''
ms.date: 06/13/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- MAXIMUM_INPUT_ATTRIBUTES parameter
- SPLIT_METHOD parameter
- MINIMUM_SUPPORT parameter
- MAXIMUM_OUTPUT_ATTRIBUTES parameter
- FORCED_REGRESSOR parameter
- decision tree algorithms [Analysis Services]
- decision trees [Analysis Services]
- COMPLEXITY_PENALTY parameter
- SCORE_METHOD parameter
ms.assetid: 1e9f7969-0aa6-465a-b3ea-57b8d1c7a1fd
author: minewiskan
ms.author: owend
ms.openlocfilehash: 0cd0cd3100d0ed1213183815ae41f17cee3baa68
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 08/04/2020
ms.locfileid: "87665662"
---
# <a name="microsoft-decision-trees-algorithm-technical-reference"></a><span data-ttu-id="35d96-102">Технический справочник по алгоритму дерева принятия решений (Майкрософт)</span><span class="sxs-lookup"><span data-stu-id="35d96-102">Microsoft Decision Trees Algorithm Technical Reference</span></span>
  <span data-ttu-id="35d96-103">Алгоритм дерева принятия решений [!INCLUDE[msCoName](../../includes/msconame-md.md)] представляет собой гибридный алгоритм, объединяющий различные методы для создания дерева и поддерживающий несколько аналитических задач, в том числе регрессию, классификацию и взаимосвязи.</span><span class="sxs-lookup"><span data-stu-id="35d96-103">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm is a hybrid algorithm that incorporates different methods for creating a tree, and supports multiple analytic tasks, including regression, classification, and association.</span></span> <span data-ttu-id="35d96-104">Алгоритм дерева принятия решений (Майкрософт) поддерживает моделирование как дискретных, так и непрерывных атрибутов.</span><span class="sxs-lookup"><span data-stu-id="35d96-104">The Microsoft Decision Trees algorithm supports modeling of both discrete and continuous attributes.</span></span>  
  
 <span data-ttu-id="35d96-105">В данном разделе описывается реализация алгоритма и настройка его поведения для различных задач. Приводятся также ссылки на дополнительную информацию о запросах к модели дерева принятия решений.</span><span class="sxs-lookup"><span data-stu-id="35d96-105">This topic explains the implementation of the algorithm, describes how to customize the behavior of the algorithm for different tasks, and provides links to additional information about querying decision tree models.</span></span>  
  
## <a name="implementation-of-the-decision-trees-algorithm"></a><span data-ttu-id="35d96-106">Реализация алгоритма дерева принятия решений</span><span class="sxs-lookup"><span data-stu-id="35d96-106">Implementation of the Decision Trees Algorithm</span></span>  
 <span data-ttu-id="35d96-107">Алгоритм дерева принятия решений (Майкрософт) применяет схему Байеса для обучения моделей причинного взаимодействия, получая приблизительные апостериорные распределения для моделей.</span><span class="sxs-lookup"><span data-stu-id="35d96-107">The Microsoft Decision Trees algorithm applies the Bayesian approach to learning causal interaction models by obtaining approximate posterior distributions for the models.</span></span> <span data-ttu-id="35d96-108">Подробное описание этого подхода можно посмотреть на веб-сайте Microsoft Research в документе [Структуры и параметры обучения](https://go.microsoft.com/fwlink/?LinkId=237640&clcid=0x409).</span><span class="sxs-lookup"><span data-stu-id="35d96-108">For a detailed explanation of this approach, see the paper on the Microsoft Research site, by [Structure and Parameter Learning](https://go.microsoft.com/fwlink/?LinkId=237640&clcid=0x409).</span></span>  
  
 <span data-ttu-id="35d96-109">Методология оценки информационной ценности *априорных вероятностей* , необходимых для обучения, основана на предположении *эквивалентности правдоподобия*.</span><span class="sxs-lookup"><span data-stu-id="35d96-109">The methodology for assessing the information value of the *priors* needed for learning is based on the assumption of *likelihood equivalence*.</span></span> <span data-ttu-id="35d96-110">Предполагается, что данные не должны способствовать различению сетевых структур, которые в противном случае представляют равносильные утверждения условной независимости.</span><span class="sxs-lookup"><span data-stu-id="35d96-110">This assumption says that data should not help to discriminate network structures that otherwise represent the same assertions of conditional independence.</span></span> <span data-ttu-id="35d96-111">Предполагается, что у каждого варианта имеется одна байесова априорная сеть и один показатель достоверности для этой сети.</span><span class="sxs-lookup"><span data-stu-id="35d96-111">Each case is assumed to have a single Bayesian prior network and a single measure of confidence for that network.</span></span>  
  
 <span data-ttu-id="35d96-112">С помощью этих априорных сетей алгоритм вычисляет относительные *апостериорные вероятности* сетевых структур на основе текущих обучающих данных и выявляет сетевые структуры с наиболее высокими апостериорными вероятностями.</span><span class="sxs-lookup"><span data-stu-id="35d96-112">Using these prior networks, the algorithm then computes the relative *posterior probabilities* of network structures given the current training data, and identifies the network structures that have the highest posterior probabilities.</span></span>  
  
 <span data-ttu-id="35d96-113">Алгоритм дерева принятия решений (Майкрософт) использует различные методы для вычисления наилучшего дерева.</span><span class="sxs-lookup"><span data-stu-id="35d96-113">The Microsoft Decision Trees algorithm uses different methods to compute the best tree.</span></span> <span data-ttu-id="35d96-114">Выбор метода зависит от задачи — это может быть линейная регрессия, классификация или анализ взаимосвязей.</span><span class="sxs-lookup"><span data-stu-id="35d96-114">The method used depends on the task, which can be linear regression, classification, or association analysis.</span></span> <span data-ttu-id="35d96-115">Единая модель может содержать несколько деревьев для различных прогнозируемых атрибутов.</span><span class="sxs-lookup"><span data-stu-id="35d96-115">A single model can contain multiple trees for different predictable attributes.</span></span> <span data-ttu-id="35d96-116">Более того, каждое дерево может содержать несколько ветвей в зависимости от того, сколько атрибутов и значений содержится в данных.</span><span class="sxs-lookup"><span data-stu-id="35d96-116">Moreover, each tree can contain multiple branches, depending on how many attributes and values there are in the data.</span></span> <span data-ttu-id="35d96-117">Форма и глубина дерева, построенного на основе конкретной модели, зависит от метода количественной оценки и от других использованных параметров.</span><span class="sxs-lookup"><span data-stu-id="35d96-117">The shape and depth of the tree built in a particular model depends on the scoring method and other parameters that were used.</span></span> <span data-ttu-id="35d96-118">Изменения в параметрах могут также влиять на разбиение узлов.</span><span class="sxs-lookup"><span data-stu-id="35d96-118">Changes in the parameters can also affect where the nodes split.</span></span>  
  
### <a name="building-the-tree"></a><span data-ttu-id="35d96-119">Построение дерева</span><span class="sxs-lookup"><span data-stu-id="35d96-119">Building the Tree</span></span>  
 <span data-ttu-id="35d96-120">Когда алгоритм дерева принятия решений (Майкрософт) создает набор возможных входных значений, он проводит *feature selection* для выявления атрибутов и значений, предоставляющих больше всего информации, и удаляет из числа рассматриваемых слишком редкие значения.</span><span class="sxs-lookup"><span data-stu-id="35d96-120">When the Microsoft Decision Trees algorithm creates the set of possible input values, it performs *feature selection* to identify the attributes and values that provide the most information, and removes from consideration the values that are very rare.</span></span> <span data-ttu-id="35d96-121">Этот алгоритм также группирует значения в *корзины*для создания группирований величин, которые можно обрабатывать вместе для оптимизации производительности.</span><span class="sxs-lookup"><span data-stu-id="35d96-121">The algorithm also groups values into *bins*, to create groupings of values that can be processed as a unit to optimize performance.</span></span>  
  
 <span data-ttu-id="35d96-122">Дерево строится посредством определения корреляции между входом и целевым выходом.</span><span class="sxs-lookup"><span data-stu-id="35d96-122">A tree is built by determining the correlations between an input and the targeted outcome.</span></span> <span data-ttu-id="35d96-123">После проведения корреляции для всех атрибутов алгоритм выявляет единственный атрибут, который лучше всего разделяет результирующие выходы.</span><span class="sxs-lookup"><span data-stu-id="35d96-123">After all the attributes have been correlated, the algorithm identifies the single attribute that most cleanly separates the outcomes.</span></span> <span data-ttu-id="35d96-124">Эта точка наилучшего разделения измеряется с помощью уравнения, которое оценивает прирост информации.</span><span class="sxs-lookup"><span data-stu-id="35d96-124">This point of the best separation is measured by using an equation that calculates information gain.</span></span> <span data-ttu-id="35d96-125">Атрибут, имеющий наилучшую оценку для прироста информации, используется для разбиения вариантов на подмножества, которые затем рекурсивно анализируются тем же алгоритмом, пока дальнейшие разбиения дерева не станут невозможными.</span><span class="sxs-lookup"><span data-stu-id="35d96-125">The attribute that has the best score for information gain is used to divide the cases into subsets, which are then recursively analyzed by the same process, until the tree cannot be split any more.</span></span>  
  
 <span data-ttu-id="35d96-126">Точное уравнение для оценки прироста информации зависит от набора параметров, применявшихся при создании алгоритма, типа данных прогнозируемого столбца и типа входных данных.</span><span class="sxs-lookup"><span data-stu-id="35d96-126">The exact equation used to evaluate information gain depends on the parameters set when you created the algorithm, the data type of the predictable column, and the data type of the input.</span></span>  
  
### <a name="discrete-and-continuous-inputs"></a><span data-ttu-id="35d96-127">Дискретные и непрерывные входные данные</span><span class="sxs-lookup"><span data-stu-id="35d96-127">Discrete and Continuous Inputs</span></span>  
 <span data-ttu-id="35d96-128">Если дискретны и прогнозируемый атрибут, и входные данные, подсчет количества результатов на каждый вход сводится к созданию матрицы и вычислению оценок для каждой ее ячейки.</span><span class="sxs-lookup"><span data-stu-id="35d96-128">When the predictable attribute is discrete and the inputs are discrete, counting the outcomes per input is a matter of creating a matrix and generating scores for each cell in the matrix.</span></span>  
  
 <span data-ttu-id="35d96-129">Однако если прогнозируемый атрибут дискретен, а входные данные непрерывны, вход непрерывных столбцов автоматически дискретизируется.</span><span class="sxs-lookup"><span data-stu-id="35d96-129">However, when the predictable attribute is discrete and the inputs are continuous, the input of the continuous columns are automatically discretized.</span></span> <span data-ttu-id="35d96-130">Можно принять значения по умолчанию и предоставить службам [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] возможность найти оптимальное количество корзин или управлять тем, каким образом дискретизируются непрерывные входы, путем настройки свойств <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationMethod%2A> и <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationBucketCount%2A> .</span><span class="sxs-lookup"><span data-stu-id="35d96-130">You can accept the default and have [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] find the optimum number of bins, or you can control the manner in which continuous inputs are discretized by setting the <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationMethod%2A> and <xref:Microsoft.AnalysisServices.ScalarMiningStructureColumn.DiscretizationBucketCount%2A> properties.</span></span> <span data-ttu-id="35d96-131">Дополнительные сведения см. в разделе [Изменение дискретизации столбца в модели интеллектуального анализа данных](change-the-discretization-of-a-column-in-a-mining-model.md).</span><span class="sxs-lookup"><span data-stu-id="35d96-131">For more information, see [Change the Discretization of a Column in a Mining Model](change-the-discretization-of-a-column-in-a-mining-model.md).</span></span>  
  
 <span data-ttu-id="35d96-132">Для непрерывных атрибутов алгоритм использует линейную регрессию для определения места разбиения дерева решений.</span><span class="sxs-lookup"><span data-stu-id="35d96-132">For continuous attributes, the algorithm uses linear regression to determine where a decision tree splits.</span></span>  
  
 <span data-ttu-id="35d96-133">Если прогнозируемый атрибут относится к непрерывному числовому типу данных, выбор компонентов применяется также и к выходам для снижения возможного числа результатов и ускорения построения модели.</span><span class="sxs-lookup"><span data-stu-id="35d96-133">When the predictable attribute is a continuous numeric data type, feature selection is applied to the outputs as well, to reduce the possible number of outcomes and build the model faster.</span></span> <span data-ttu-id="35d96-134">Настроив параметр MAXIMUM_OUTPUT_ATTRIBUTES, можно изменить пороговое значение для выбора компонентов и таким образом увеличить или снизить число возможных величин.</span><span class="sxs-lookup"><span data-stu-id="35d96-134">You can change the threshold for feature selection and thereby increase or decrease the number of possible values by setting the MAXIMUM_OUTPUT_ATTRIBUTES parameter.</span></span>  
  
 <span data-ttu-id="35d96-135">Более подробное объяснение того, как алгоритм дерева принятия решений [!INCLUDE[msCoName](../../includes/msconame-md.md)] работает с дискретными прогнозируемыми столбцами, см. в документе [Learning Bayesian Networks: The Combination of Knowledge and Statistical Data](https://go.microsoft.com/fwlink/?LinkId=45963)(Обучаемые байесовы сети: сочетание знаний и статистических данных).</span><span class="sxs-lookup"><span data-stu-id="35d96-135">For a more detained explanation about how the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm works with discrete predictable columns, see [Learning Bayesian Networks: The Combination of Knowledge and Statistical Data](https://go.microsoft.com/fwlink/?LinkId=45963).</span></span> <span data-ttu-id="35d96-136">Дополнительные сведения о том, каким образом алгоритм дерева принятия решений [!INCLUDE[msCoName](../../includes/msconame-md.md)] работает с непрерывным прогнозируемым столбцом, см. в приложении к статье [Модели деревьев с авторегрессией для анализа временных рядов](https://go.microsoft.com/fwlink/?LinkId=45966).</span><span class="sxs-lookup"><span data-stu-id="35d96-136">For more information about how the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm works with a continuous predictable column, see the appendix of [Autoregressive Tree Models for Time-Series Analysis](https://go.microsoft.com/fwlink/?LinkId=45966).</span></span>  
  
### <a name="scoring-methods-and-feature-selection"></a><span data-ttu-id="35d96-137">Методы количественной оценки и выбор компонентов</span><span class="sxs-lookup"><span data-stu-id="35d96-137">Scoring Methods and Feature Selection</span></span>  
 <span data-ttu-id="35d96-138">В алгоритме дерева принятия решений (Майкрософт) предусмотрены три формулы для вычисления прироста информации: энтропия Шеннона, метод Байеса с априорной оценкой K2 и байесовская сеть с однородной априорной оценкой Дирихле.</span><span class="sxs-lookup"><span data-stu-id="35d96-138">The Microsoft Decision Trees algorithm offers three formulas for scoring information gain: Shannon's entropy, Bayesian network with K2 prior, and Bayesian network with a uniform Dirichlet distribution of priors.</span></span> <span data-ttu-id="35d96-139">Все три метода широко применяются для интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="35d96-139">All three methods are well established in the data mining field.</span></span> <span data-ttu-id="35d96-140">Рекомендуется поэкспериментировать с различными параметрами и методами количественной оценки, чтобы понять, какие из них дают наилучшие результаты.</span><span class="sxs-lookup"><span data-stu-id="35d96-140">We recommend that you experiment with different parameters and scoring methods to determine which provides the best results.</span></span> <span data-ttu-id="35d96-141">Дополнительные сведения об этих методах количественной оценки см. в разделе [Feature Selection](../../sql-server/install/feature-selection.md).</span><span class="sxs-lookup"><span data-stu-id="35d96-141">For more information about these scoring methods, see [Feature Selection](../../sql-server/install/feature-selection.md).</span></span>  
  
 <span data-ttu-id="35d96-142">Выбор компонентов автоматически применяется всеми алгоритмами интеллектуального анализа данных служб [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] для улучшения качества анализа и снижения вычислительной нагрузки.</span><span class="sxs-lookup"><span data-stu-id="35d96-142">All [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] data mining algorithms automatically use feature selection to improve analysis and reduce processing load.</span></span> <span data-ttu-id="35d96-143">Метод, применяемый для выбора компонентов, зависит от алгоритма, который был использован при создании модели.</span><span class="sxs-lookup"><span data-stu-id="35d96-143">The method used for feature selection depends on the algorithm that is used to build the model.</span></span> <span data-ttu-id="35d96-144">Выбором компонентов в модели дерева решений управляют следующие параметры алгоритма: MAXIMUM_INPUT_ATTRIBUTES и MAXIMUM_OUTPUT.</span><span class="sxs-lookup"><span data-stu-id="35d96-144">The algorithm parameters that control feature selection for a decision trees model are MAXIMUM_INPUT_ATTRIBUTES and MAXIMUM_OUTPUT.</span></span>  
  
|<span data-ttu-id="35d96-145">Алгоритм</span><span class="sxs-lookup"><span data-stu-id="35d96-145">Algorithm</span></span>|<span data-ttu-id="35d96-146">Метод анализа</span><span class="sxs-lookup"><span data-stu-id="35d96-146">Method of analysis</span></span>|<span data-ttu-id="35d96-147">Комментарии</span><span class="sxs-lookup"><span data-stu-id="35d96-147">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="35d96-148">Деревья решений</span><span class="sxs-lookup"><span data-stu-id="35d96-148">Decision Trees</span></span>|<span data-ttu-id="35d96-149">Оценка интересности</span><span class="sxs-lookup"><span data-stu-id="35d96-149">Interestingness score</span></span><br /><br /> <span data-ttu-id="35d96-150">Энтропия Шеннона</span><span class="sxs-lookup"><span data-stu-id="35d96-150">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="35d96-151">Алгоритм Байеса с априорной оценкой K2</span><span class="sxs-lookup"><span data-stu-id="35d96-151">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="35d96-152">Эквивалент Дирихле метода Байеса с однородной априорной оценкой (выбор по умолчанию)</span><span class="sxs-lookup"><span data-stu-id="35d96-152">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="35d96-153">Если какие-либо столбцы содержат недвоичные непрерывные значения, то оценка интересности используется для всех столбцов в целях обеспечения согласованности.</span><span class="sxs-lookup"><span data-stu-id="35d96-153">If any columns contain non-binary continuous values, the interestingness score is used for all columns, to ensure consistency.</span></span> <span data-ttu-id="35d96-154">В противном случае используется предусмотренный по умолчанию или указанный метод.</span><span class="sxs-lookup"><span data-stu-id="35d96-154">Otherwise, the default or specified method is used.</span></span>|  
|<span data-ttu-id="35d96-155">Линейная регрессия</span><span class="sxs-lookup"><span data-stu-id="35d96-155">Linear Regression</span></span>|<span data-ttu-id="35d96-156">Оценка интересности</span><span class="sxs-lookup"><span data-stu-id="35d96-156">Interestingness score</span></span>|<span data-ttu-id="35d96-157">В алгоритме линейной регрессии применяется только оценка интересности, поскольку этот алгоритм поддерживает лишь непрерывные столбцы.</span><span class="sxs-lookup"><span data-stu-id="35d96-157">Linear Regression only uses interestingness, because it only supports continuous columns.</span></span>|  
  
### <a name="scalability-and-performance"></a><span data-ttu-id="35d96-158">Масштабируемость и производительность</span><span class="sxs-lookup"><span data-stu-id="35d96-158">Scalability and Performance</span></span>  
 <span data-ttu-id="35d96-159">Классификация является важной стратегией интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="35d96-159">Classification is an important data mining strategy.</span></span> <span data-ttu-id="35d96-160">Обычно количество информации, нужное для классификации вариантов, растет прямо пропорционально количеству входных записей.</span><span class="sxs-lookup"><span data-stu-id="35d96-160">Generally, the amount of information that is needed to classify the cases grows in direct proportion to the number of input records.</span></span> <span data-ttu-id="35d96-161">Это ограничивает объем данных, поддающихся классификации.</span><span class="sxs-lookup"><span data-stu-id="35d96-161">This limits the size of the data that can be classified.</span></span> <span data-ttu-id="35d96-162">Алгоритм дерева принятия решений (Майкрософт) использует следующие методы для решения упомянутых выше проблем, улучшения производительности и устранения ограничений памяти.</span><span class="sxs-lookup"><span data-stu-id="35d96-162">The Microsoft Decision Trees algorithm using uses the following methods to resolve these problems, improve performance, and eliminate memory restrictions:</span></span>  
  
-   <span data-ttu-id="35d96-163">Выбор компонентов для оптимизации выбора атрибутов.</span><span class="sxs-lookup"><span data-stu-id="35d96-163">Feature selection to optimize the selection of attributes.</span></span>  
  
-   <span data-ttu-id="35d96-164">Вычисление байесовских оценок для управления ростом дерева.</span><span class="sxs-lookup"><span data-stu-id="35d96-164">Bayesian scoring to control tree growth.</span></span>  
  
-   <span data-ttu-id="35d96-165">Оптимизация разделения на корзины для непрерывных атрибутов.</span><span class="sxs-lookup"><span data-stu-id="35d96-165">Optimization of binning for continuous attributes.</span></span>  
  
-   <span data-ttu-id="35d96-166">Динамическое группирование входных значений для определения самых важных данных.</span><span class="sxs-lookup"><span data-stu-id="35d96-166">Dynamic grouping of input values to determine the most important values.</span></span>  
  
 <span data-ttu-id="35d96-167">Алгоритм дерева принятия решений (Майкрософт) — быстрый, хорошо масштабируемый алгоритм, созданный для удобства параллелизации, а это означает, что все процессоры системы могут работать вместе для создания единой согласованной модели.</span><span class="sxs-lookup"><span data-stu-id="35d96-167">The Microsoft Decision Trees algorithm is fast and scalable, and has been designed to be easily parallelized, meaning that all processors work together to build a single, consistent model.</span></span> <span data-ttu-id="35d96-168">Сочетание этих характеристик делает классификатор на основе дерева принятия решений идеальным средством для интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="35d96-168">The combination of these characteristics makes the decision-tree classifier an ideal tool for data mining.</span></span>  
  
 <span data-ttu-id="35d96-169">Если существуют жесткие ограничения на производительность, можно попробовать улучшить время обработки в процессе обучения модели дерева принятия решений с помощью следующих методов.</span><span class="sxs-lookup"><span data-stu-id="35d96-169">If performance constraints are severe, you might be able to improve processing time during the training of a decision tree model by using the following methods.</span></span> <span data-ttu-id="35d96-170">Однако нужно хорошо понимать, что отбрасывание атрибутов для улучшения производительности обработки изменит результаты модели, и, возможно, сделает ее менее репрезентативной для всей совокупности данных.</span><span class="sxs-lookup"><span data-stu-id="35d96-170">However, if you do so, be aware that eliminating attributes to improve processing performance will change the results of the model, and possibly make it less representative of the total population.</span></span>  
  
-   <span data-ttu-id="35d96-171">Увеличение параметра COMPLEXITY_PENALTY для ограничения роста дерева.</span><span class="sxs-lookup"><span data-stu-id="35d96-171">Increase the value of the COMPLEXITY_PENALTY parameter to limit tree growth.</span></span>  
  
-   <span data-ttu-id="35d96-172">Ограничение количества элементов в модели взаимосвязей для ограничения количества создаваемых деревьев.</span><span class="sxs-lookup"><span data-stu-id="35d96-172">Limit the number of items in association models to limit the number of trees that are built.</span></span>  
  
-   <span data-ttu-id="35d96-173">Увеличение параметра MINIMUM_SUPPORT во избежание создания лжевзаимосвязи.</span><span class="sxs-lookup"><span data-stu-id="35d96-173">Increase the value of the MINIMUM_SUPPORT parameter to avoid overfitting.</span></span>  
  
-   <span data-ttu-id="35d96-174">Сокращение количества дискретных значений всех атрибутов до 10 или менее.</span><span class="sxs-lookup"><span data-stu-id="35d96-174">Restrict the number of discrete values for any attribute to 10 or less.</span></span> <span data-ttu-id="35d96-175">Можно попробовать группировать значения по-разному в разных моделях.</span><span class="sxs-lookup"><span data-stu-id="35d96-175">You might try grouping values in different ways in different models.</span></span>  
  
    > [!NOTE]  
    >  <span data-ttu-id="35d96-176">Средства просмотра данных, доступные в службах  [!INCLUDE[ssISCurrent](../../includes/ssiscurrent-md.md)] , можно использовать для визуализации распределения значений в данных и соответствующего группирования этих значений до начала интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="35d96-176">You can use the data exploration tools available in  [!INCLUDE[ssISCurrent](../../includes/ssiscurrent-md.md)] to visualize the distribution of values in your data and group your values appropriately before beginning data mining.</span></span> <span data-ttu-id="35d96-177">Дополнительные сведения см. в статье [Задачи профилирования и просмотра данных](../../integration-services/control-flow/data-profiling-task-and-viewer.md).</span><span class="sxs-lookup"><span data-stu-id="35d96-177">For more information, see [Data Profiling Task and Viewer](../../integration-services/control-flow/data-profiling-task-and-viewer.md).</span></span> <span data-ttu-id="35d96-178">Можно также исследовать, группировать и переразмечать данные в программе Microsoft Excel с помощью [надстроек интеллектуального анализа данных для Excel 2007](https://www.microsoft.com/download/details.aspx?id=8569).</span><span class="sxs-lookup"><span data-stu-id="35d96-178">You can also use the [Data Mining Add-ins for Excel 2007](https://www.microsoft.com/download/details.aspx?id=8569), to explore, group and relabel data in Microsoft Excel.</span></span>  
  
## <a name="customizing-the-decision-trees-algorithm"></a><span data-ttu-id="35d96-179">Настройка алгоритма дерева принятия решений</span><span class="sxs-lookup"><span data-stu-id="35d96-179">Customizing the Decision Trees Algorithm</span></span>  
 <span data-ttu-id="35d96-180">Алгоритм дерева принятия решений [!INCLUDE[msCoName](../../includes/msconame-md.md)] поддерживает несколько параметров, влияющих на производительность и точность получающейся в результате модели интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="35d96-180">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports parameters that affect the performance and accuracy of the resulting mining model.</span></span> <span data-ttu-id="35d96-181">Можно также изменять способ обработки данных, устанавливая на столбцах модели интеллектуального анализа данных или структуры интеллектуального анализа данных флаги модели.</span><span class="sxs-lookup"><span data-stu-id="35d96-181">You can also set modeling flags on the mining model columns or mining structure columns to control the way that data is processed.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="35d96-182">Алгоритм дерева принятия решений (Майкрософт) доступен во всех выпусках [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)], однако некоторые дополнительные параметры для настройки работы этого алгоритма доступны только в специальных выпусках [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)].</span><span class="sxs-lookup"><span data-stu-id="35d96-182">The Microsoft Decision Trees algorithm is available in all editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]; however, some advanced parameters for customizing the behavior of the Microsoft Decision Trees algorithm are available for use only in specific editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)].</span></span> <span data-ttu-id="35d96-183">Список функций, поддерживаемых различными выпусками [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] , см. [в разделе функции, поддерживаемые различными выпусками SQL Server 2012](https://go.microsoft.com/fwlink/?linkid=232473) ( https://go.microsoft.com/fwlink/?linkid=232473) .</span><span class="sxs-lookup"><span data-stu-id="35d96-183">For a list of features that are supported by the editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)], see [Features Supported by the Editions of SQL Server 2012](https://go.microsoft.com/fwlink/?linkid=232473) (https://go.microsoft.com/fwlink/?linkid=232473).</span></span>  
  
### <a name="setting-algorithm-parameters"></a><span data-ttu-id="35d96-184">Задание параметров алгоритма</span><span class="sxs-lookup"><span data-stu-id="35d96-184">Setting Algorithm Parameters</span></span>  
 <span data-ttu-id="35d96-185">В следующей таблице описаны параметры, которые можно использовать с алгоритмом временных рядов [!INCLUDE[msCoName](../../includes/msconame-md.md)] .</span><span class="sxs-lookup"><span data-stu-id="35d96-185">The following table describes the parameters that you can use with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm.</span></span>  
  
 <span data-ttu-id="35d96-186">*COMPLEXITY_PENALTY*</span><span class="sxs-lookup"><span data-stu-id="35d96-186">*COMPLEXITY_PENALTY*</span></span>  
 <span data-ttu-id="35d96-187">Управляет ростом дерева решений.</span><span class="sxs-lookup"><span data-stu-id="35d96-187">Controls the growth of the decision tree.</span></span> <span data-ttu-id="35d96-188">Низкое значение увеличивает количество разбиений, а высокое количество — уменьшает.</span><span class="sxs-lookup"><span data-stu-id="35d96-188">A low value increases the number of splits, and a high value decreases the number of splits.</span></span> <span data-ttu-id="35d96-189">Значение по умолчанию основано на количестве атрибутов для конкретной модели, как описано в следующем списке.</span><span class="sxs-lookup"><span data-stu-id="35d96-189">The default value is based on the number of attributes for a particular model, as described in the following list:</span></span>  
  
-   <span data-ttu-id="35d96-190">Для атрибутов с 1 до 9 значением по умолчанию является 0,5.</span><span class="sxs-lookup"><span data-stu-id="35d96-190">For 1 through 9 attributes, the default is 0.5.</span></span>  
  
-   <span data-ttu-id="35d96-191">Для атрибутов с 10 до 99 значением по умолчанию является 0,9.</span><span class="sxs-lookup"><span data-stu-id="35d96-191">For 10 through 99 attributes, the default is 0.9.</span></span>  
  
-   <span data-ttu-id="35d96-192">Для 100 или более атрибутов значением по умолчанию является 0,99.</span><span class="sxs-lookup"><span data-stu-id="35d96-192">For 100 or more attributes, the default is 0.99.</span></span>  
  
 <span data-ttu-id="35d96-193">*FORCE_REGRESSOR*</span><span class="sxs-lookup"><span data-stu-id="35d96-193">*FORCE_REGRESSOR*</span></span>  
 <span data-ttu-id="35d96-194">Вынуждает алгоритм использовать указанные столбцы в качестве регрессоров, не обращая внимания на важность столбцов, вычисленную алгоритмом.</span><span class="sxs-lookup"><span data-stu-id="35d96-194">Forces the algorithm to use the specified columns as regressors, regardless of the importance of the columns as calculated by the algorithm.</span></span> <span data-ttu-id="35d96-195">Этот параметр используется только для деревьев решений, прогнозирующих непрерывный атрибут.</span><span class="sxs-lookup"><span data-stu-id="35d96-195">This parameter is only used for decision trees that are predicting a continuous attribute.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="35d96-196">Установив этот параметр, можно заставить алгоритм попытаться использовать данный атрибут в качестве регрессора.</span><span class="sxs-lookup"><span data-stu-id="35d96-196">By setting this parameter, you force the algorithm to try to use the attribute as a regressor.</span></span> <span data-ttu-id="35d96-197">Однако будет ли этот атрибут действительно использован как регрессор в результирующей модели, зависит от результатов анализа.</span><span class="sxs-lookup"><span data-stu-id="35d96-197">However, whether the attribute is actually used as a regressor in the final model depends on the results of analysis.</span></span> <span data-ttu-id="35d96-198">Можно выяснить, какие столбцы были использованы в качестве регрессора, с помощью запроса содержимого модели.</span><span class="sxs-lookup"><span data-stu-id="35d96-198">You can find out which columns were used as regressors by querying the model content.</span></span>  
  
 <span data-ttu-id="35d96-199">[Доступно только в некоторых выпусках [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] ]</span><span class="sxs-lookup"><span data-stu-id="35d96-199">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)] ]</span></span>  
  
 <span data-ttu-id="35d96-200">*MAXIMUM_INPUT_ATTRIBUTES*</span><span class="sxs-lookup"><span data-stu-id="35d96-200">*MAXIMUM_INPUT_ATTRIBUTES*</span></span>  
 <span data-ttu-id="35d96-201">Определяет количество входных атрибутов, которые алгоритм может обработать перед вызовом выбора компонентов.</span><span class="sxs-lookup"><span data-stu-id="35d96-201">Defines the number of input attributes that the algorithm can handle before it invokes feature selection.</span></span>  
  
 <span data-ttu-id="35d96-202">Значение по умолчанию — 255.</span><span class="sxs-lookup"><span data-stu-id="35d96-202">The default is 255.</span></span>  
  
 <span data-ttu-id="35d96-203">Установите значение 0, чтобы отключить выбор компонентов.</span><span class="sxs-lookup"><span data-stu-id="35d96-203">Set this value to 0 to turn off feature selection.</span></span>  
  
 <span data-ttu-id="35d96-204">[Доступно только в некоторых выпусках [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span><span class="sxs-lookup"><span data-stu-id="35d96-204">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span></span>  
  
 <span data-ttu-id="35d96-205">*MAXIMUM_OUTPUT_ATTRIBUTES*</span><span class="sxs-lookup"><span data-stu-id="35d96-205">*MAXIMUM_OUTPUT_ATTRIBUTES*</span></span>  
 <span data-ttu-id="35d96-206">Определяет количество выходных атрибутов, которые алгоритм может обработать перед вызовом выбора компонентов.</span><span class="sxs-lookup"><span data-stu-id="35d96-206">Defines the number of output attributes that the algorithm can handle before it invokes feature selection.</span></span>  
  
 <span data-ttu-id="35d96-207">Значение по умолчанию — 255.</span><span class="sxs-lookup"><span data-stu-id="35d96-207">The default is 255.</span></span>  
  
 <span data-ttu-id="35d96-208">Установите значение 0, чтобы отключить выбор компонентов.</span><span class="sxs-lookup"><span data-stu-id="35d96-208">Set this value to 0 to turn off feature selection.</span></span>  
  
 <span data-ttu-id="35d96-209">[Доступно только в некоторых выпусках [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span><span class="sxs-lookup"><span data-stu-id="35d96-209">[Available only in some editions of [!INCLUDE[ssNoVersion](../../includes/ssnoversion-md.md)]]</span></span>  
  
 <span data-ttu-id="35d96-210">*MINIMUM_SUPPORT*</span><span class="sxs-lookup"><span data-stu-id="35d96-210">*MINIMUM_SUPPORT*</span></span>  
 <span data-ttu-id="35d96-211">Определяет минимальное количество конечных вариантов, необходимых для формирования разбиения в дереве решений.</span><span class="sxs-lookup"><span data-stu-id="35d96-211">Determines the minimum number of leaf cases that is required to generate a split in the decision tree.</span></span>  
  
 <span data-ttu-id="35d96-212">Значение по умолчанию равно 10.</span><span class="sxs-lookup"><span data-stu-id="35d96-212">The default is 10.</span></span>  
  
 <span data-ttu-id="35d96-213">Для очень больших наборов данных это значение, возможно, придется увеличить, чтобы избежать чрезмерно тщательного обучения.</span><span class="sxs-lookup"><span data-stu-id="35d96-213">You may need to increase this value if the dataset is very large, to avoid overtraining.</span></span>  
  
 <span data-ttu-id="35d96-214">*SCORE_METHOD*</span><span class="sxs-lookup"><span data-stu-id="35d96-214">*SCORE_METHOD*</span></span>  
 <span data-ttu-id="35d96-215">Определяет метод, используемый для вычисления коэффициента разбиения.</span><span class="sxs-lookup"><span data-stu-id="35d96-215">Determines the method that is used to calculate the split score.</span></span> <span data-ttu-id="35d96-216">Доступны следующие варианты:</span><span class="sxs-lookup"><span data-stu-id="35d96-216">The following options are available:</span></span>  
  
|<span data-ttu-id="35d96-217">ID</span><span class="sxs-lookup"><span data-stu-id="35d96-217">ID</span></span>|<span data-ttu-id="35d96-218">Имя</span><span class="sxs-lookup"><span data-stu-id="35d96-218">Name</span></span>|  
|--------|----------|  
|<span data-ttu-id="35d96-219">1</span><span class="sxs-lookup"><span data-stu-id="35d96-219">1</span></span>|<span data-ttu-id="35d96-220">Энтропия</span><span class="sxs-lookup"><span data-stu-id="35d96-220">Entropy</span></span>|  
|<span data-ttu-id="35d96-221">3</span><span class="sxs-lookup"><span data-stu-id="35d96-221">3</span></span>|<span data-ttu-id="35d96-222">Алгоритм Байеса с априорной оценкой K2</span><span class="sxs-lookup"><span data-stu-id="35d96-222">Bayesian with K2 Prior</span></span>|  
|<span data-ttu-id="35d96-223">4</span><span class="sxs-lookup"><span data-stu-id="35d96-223">4</span></span>|<span data-ttu-id="35d96-224">Эквивалент Дирихле метода Байеса (BDE) с однородной априорной оценкой</span><span class="sxs-lookup"><span data-stu-id="35d96-224">Bayesian Dirichlet Equivalent (BDE) with uniform prior</span></span><br /><br /> <span data-ttu-id="35d96-225">(по умолчанию).</span><span class="sxs-lookup"><span data-stu-id="35d96-225">(default)</span></span>|  
  
 <span data-ttu-id="35d96-226">Значение по умолчанию — 4 (или BDE).</span><span class="sxs-lookup"><span data-stu-id="35d96-226">The default is 4, or BDE.</span></span>  
  
 <span data-ttu-id="35d96-227">Обзор трех данных методов количественной оценки см. в разделе [Feature Selection](../../sql-server/install/feature-selection.md).</span><span class="sxs-lookup"><span data-stu-id="35d96-227">For an explanation of these scoring methods, see [Feature Selection](../../sql-server/install/feature-selection.md).</span></span>  
  
 <span data-ttu-id="35d96-228">*SPLIT_METHOD*</span><span class="sxs-lookup"><span data-stu-id="35d96-228">*SPLIT_METHOD*</span></span>  
 <span data-ttu-id="35d96-229">Определяет метод, используемый для разбиения узла.</span><span class="sxs-lookup"><span data-stu-id="35d96-229">Determines the method that is used to split the node.</span></span> <span data-ttu-id="35d96-230">Доступны следующие варианты:</span><span class="sxs-lookup"><span data-stu-id="35d96-230">The following options are available:</span></span>  
  
|<span data-ttu-id="35d96-231">ID</span><span class="sxs-lookup"><span data-stu-id="35d96-231">ID</span></span>|<span data-ttu-id="35d96-232">Имя</span><span class="sxs-lookup"><span data-stu-id="35d96-232">Name</span></span>|  
|--------|----------|  
|<span data-ttu-id="35d96-233">1</span><span class="sxs-lookup"><span data-stu-id="35d96-233">1</span></span>|<span data-ttu-id="35d96-234">**Binary:** указывает, что независимо от реального числа значений атрибута дерево следует разбить на две ветви.</span><span class="sxs-lookup"><span data-stu-id="35d96-234">**Binary:** Indicates that regardless of the actual number of values for the attribute, the tree should be split into two branches.</span></span>|  
|<span data-ttu-id="35d96-235">2</span><span class="sxs-lookup"><span data-stu-id="35d96-235">2</span></span>|<span data-ttu-id="35d96-236">**Complete:** указывает, что в дереве можно создавать столько разбиений, сколько существует значений атрибута.</span><span class="sxs-lookup"><span data-stu-id="35d96-236">**Complete:** Indicates that the tree can create as many splits as there are attribute values.</span></span>|  
|<span data-ttu-id="35d96-237">3</span><span class="sxs-lookup"><span data-stu-id="35d96-237">3</span></span>|<span data-ttu-id="35d96-238">**Both:** указывает, что службы Analysis Services могут определять, какое разбиение лучше использовать — бинарное или полное.</span><span class="sxs-lookup"><span data-stu-id="35d96-238">**Both:** Specifies that Analysis Services can determine whether a binary or complete split should be used to produce the best results.</span></span>|  
  
 <span data-ttu-id="35d96-239">По умолчанию используется значение 3.</span><span class="sxs-lookup"><span data-stu-id="35d96-239">The default is 3.</span></span>  
  
### <a name="modeling-flags"></a><span data-ttu-id="35d96-240">Флаги моделирования</span><span class="sxs-lookup"><span data-stu-id="35d96-240">Modeling Flags</span></span>  
 <span data-ttu-id="35d96-241">Алгоритм деревьев решений [!INCLUDE[msCoName](../../includes/msconame-md.md)] поддерживает следующие флаги модели.</span><span class="sxs-lookup"><span data-stu-id="35d96-241">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports the following modeling flags.</span></span> <span data-ttu-id="35d96-242">Чтобы задать порядок обработки в ходе анализа значений в каждом столбце, во время создания структуры или модели интеллектуального анализа данных определяются флаги модели.</span><span class="sxs-lookup"><span data-stu-id="35d96-242">When you create the mining structure or mining model, you define modeling flags to specify how values in each column are handled during analysis.</span></span> <span data-ttu-id="35d96-243">Дополнительные сведения см. в разделе [Флаги моделирования (интеллектуальный анализ данных)](modeling-flags-data-mining.md).</span><span class="sxs-lookup"><span data-stu-id="35d96-243">For more information, see [Modeling Flags &#40;Data Mining&#41;](modeling-flags-data-mining.md).</span></span>  
  
|<span data-ttu-id="35d96-244">Флаг моделирования</span><span class="sxs-lookup"><span data-stu-id="35d96-244">Modeling Flag</span></span>|<span data-ttu-id="35d96-245">Описание</span><span class="sxs-lookup"><span data-stu-id="35d96-245">Description</span></span>|  
|-------------------|-----------------|  
|<span data-ttu-id="35d96-246">MODEL_EXISTENCE_ONLY</span><span class="sxs-lookup"><span data-stu-id="35d96-246">MODEL_EXISTENCE_ONLY</span></span>|<span data-ttu-id="35d96-247">Столбец будет обрабатываться так, как будто у него два возможных состояния: `Missing` и `Existing`.</span><span class="sxs-lookup"><span data-stu-id="35d96-247">Means that the column will be treated as having two possible states: `Missing` and `Existing`.</span></span> <span data-ttu-id="35d96-248">NULL означает отсутствие значения.</span><span class="sxs-lookup"><span data-stu-id="35d96-248">A null is a missing value.</span></span><br /><br /> <span data-ttu-id="35d96-249">Применяется к столбцам модели интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="35d96-249">Applies to mining model columns.</span></span>|  
|<span data-ttu-id="35d96-250">NOT NULL</span><span class="sxs-lookup"><span data-stu-id="35d96-250">NOT NULL</span></span>|<span data-ttu-id="35d96-251">Указывает, что столбец не может принимать значение NULL.</span><span class="sxs-lookup"><span data-stu-id="35d96-251">Indicates that the column cannot contain a null.</span></span> <span data-ttu-id="35d96-252">Если во время обучения модели службы Analysis Services обнаружат значение NULL, возникнет ошибка.</span><span class="sxs-lookup"><span data-stu-id="35d96-252">An error will result if Analysis Services encounters a null during model training.</span></span><br /><br /> <span data-ttu-id="35d96-253">Применяется к столбцам структуры интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="35d96-253">Applies to mining structure columns.</span></span>|  
  
### <a name="regressors-in-decision-tree-models"></a><span data-ttu-id="35d96-254">Регрессоры в моделях дерева принятия решений (Microsoft)</span><span class="sxs-lookup"><span data-stu-id="35d96-254">Regressors in Decision Tree Models</span></span>  
 <span data-ttu-id="35d96-255">Даже если не используется алгоритм линейной регрессии [!INCLUDE[msCoName](../../includes/msconame-md.md)] , любая модель дерева решений, в которой есть непрерывные числовые входы и выходы, может содержать узлы, представляющие регрессию применительно к непрерывному атрибуту.</span><span class="sxs-lookup"><span data-stu-id="35d96-255">Even if you do not use the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Linear Regression algorithm, any decision tree model that has continuous numeric inputs and outputs can potentially include nodes that represent a regression on a continuous attribute.</span></span>  
  
 <span data-ttu-id="35d96-256">Не обязательно указывать, что столбец непрерывных числовых данных представляет собой регрессор.</span><span class="sxs-lookup"><span data-stu-id="35d96-256">You do not need to specify that a column of continuous numeric data represents a regressor.</span></span> <span data-ttu-id="35d96-257">Алгоритм дерева принятия решений [!INCLUDE[msCoName](../../includes/msconame-md.md)] автоматически использует этот столбец как потенциальный регрессор и секционирует набор данных на области со значимыми шаблонами, даже если для столбца не задан флаг REGRESSOR.</span><span class="sxs-lookup"><span data-stu-id="35d96-257">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm will automatically use the column as a potential regressor and partition the dataset into regions with meaningful patterns even if you do not set the REGRESSOR flag on the column.</span></span>  
  
 <span data-ttu-id="35d96-258">Однако можно применить параметр FORCE_REGRESSOR, чтобы гарантировать использование конкретного регрессора в алгоритме.</span><span class="sxs-lookup"><span data-stu-id="35d96-258">However, you can use the FORCE_REGRESSOR parameter to guarantee that the algorithm will use a particular regressor.</span></span> <span data-ttu-id="35d96-259">Этот параметр может применяться только с алгоритмом дерева принятия решений [!INCLUDE[msCoName](../../includes/msconame-md.md)] и алгоритмом линейной регрессии [!INCLUDE[msCoName](../../includes/msconame-md.md)] .</span><span class="sxs-lookup"><span data-stu-id="35d96-259">This parameter can be used only with the [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees and [!INCLUDE[msCoName](../../includes/msconame-md.md)] Linear Regression algorithms.</span></span> <span data-ttu-id="35d96-260">При установке флага моделирования алгоритм попытается найти регрессионные уравнения вида a \* C1 + b \* C2 +... для соответствия шаблонам в узлах дерева.</span><span class="sxs-lookup"><span data-stu-id="35d96-260">When you set the modeling flag, the algorithm will try to find regression equations of the form a\*C1 + b\*C2 + ... to fit the patterns in the nodes of the tree.</span></span> <span data-ttu-id="35d96-261">Вычисляется сумма остатков, и, если отклонение слишком велико, принудительно выполняется разбиение дерева.</span><span class="sxs-lookup"><span data-stu-id="35d96-261">The sum of the residuals is calculated, and if the deviation is too great, a split is forced in the tree.</span></span>  
  
 <span data-ttu-id="35d96-262">Например, если осуществляется прогноз поведения клиента в процессе покупки с использованием дохода, **Income** , в качестве атрибута и на соответствующем столбце устанавливается флаг модели REGRESSOR, то в алгоритме вначале предпринимается попытка выполнить подгонку значений **Income** с применением стандартной формулы регрессии.</span><span class="sxs-lookup"><span data-stu-id="35d96-262">For example, if you are predicting customer purchasing behavior using **Income** as an attribute, and set the REGRESSOR modeling flag on the column, the algorithm will first try to fit the **Income** values by using a standard regression formula.</span></span> <span data-ttu-id="35d96-263">Если отклонение слишком велико, то происходит отказ от применения формулы регрессии, и разбиение дерева осуществляется по какому-то другому атрибуту.</span><span class="sxs-lookup"><span data-stu-id="35d96-263">If the deviation is too great, the regression formula is abandoned and the tree will be split on another attribute.</span></span> <span data-ttu-id="35d96-264">Затем алгоритм дерева решений пытается осуществить подгонку регрессора к доходу в каждой из ветвей, полученных после разбиения.</span><span class="sxs-lookup"><span data-stu-id="35d96-264">The decision tree algorithm will then try to fit a regressor for income in each of the branches after the split.</span></span>  
  
## <a name="requirements"></a><span data-ttu-id="35d96-265">Требования</span><span class="sxs-lookup"><span data-stu-id="35d96-265">Requirements</span></span>  
 <span data-ttu-id="35d96-266">Модель дерева решений должна содержать ключевой столбец, входные столбцы и по крайней мере один прогнозируемый столбец.</span><span class="sxs-lookup"><span data-stu-id="35d96-266">A decision tree model must contain a key column, input columns, and at least one predictable column.</span></span>  
  
### <a name="input-and-predictable-columns"></a><span data-ttu-id="35d96-267">Входные и прогнозируемые столбцы</span><span class="sxs-lookup"><span data-stu-id="35d96-267">Input and Predictable Columns</span></span>  
 <span data-ttu-id="35d96-268">Алгоритм дерева решений [!INCLUDE[msCoName](../../includes/msconame-md.md)] поддерживает определенные входные столбцы данных и прогнозируемые столбцы, которые перечислены ниже в таблице.</span><span class="sxs-lookup"><span data-stu-id="35d96-268">The [!INCLUDE[msCoName](../../includes/msconame-md.md)] Decision Trees algorithm supports the specific input columns and predictable columns that are listed in the following table.</span></span> <span data-ttu-id="35d96-269">Дополнительные сведения о значении типов содержимого в применении к модели интеллектуального анализа данных см. в разделе [Типы содержимого (интеллектуальный анализ данных)](content-types-data-mining.md).</span><span class="sxs-lookup"><span data-stu-id="35d96-269">For more information about what the content types mean when used in a mining model, see [Content Types &#40;Data Mining&#41;](content-types-data-mining.md).</span></span>  
  
|<span data-ttu-id="35d96-270">Столбец</span><span class="sxs-lookup"><span data-stu-id="35d96-270">Column</span></span>|<span data-ttu-id="35d96-271">Типы содержимого</span><span class="sxs-lookup"><span data-stu-id="35d96-271">Content types</span></span>|  
|------------|-------------------|  
|<span data-ttu-id="35d96-272">Входной атрибут</span><span class="sxs-lookup"><span data-stu-id="35d96-272">Input attribute</span></span>|<span data-ttu-id="35d96-273">Continuous, Cyclical, Discrete, Discretized, Key, Ordered, Table</span><span class="sxs-lookup"><span data-stu-id="35d96-273">Continuous, Cyclical, Discrete, Discretized, Key, Ordered, Table</span></span>|  
|<span data-ttu-id="35d96-274">Прогнозируемый атрибут</span><span class="sxs-lookup"><span data-stu-id="35d96-274">Predictable attribute</span></span>|<span data-ttu-id="35d96-275">Continuous, Cyclical, Discrete, Discretized, Ordered, Table</span><span class="sxs-lookup"><span data-stu-id="35d96-275">Continuous, Cyclical, Discrete, Discretized, Ordered, Table</span></span>|  
  
> [!NOTE]  
>  <span data-ttu-id="35d96-276">Типы содержимого Cyclical и Ordered поддерживаются, но алгоритм обрабатывает их как дискретные величины и не производит их особой обработки.</span><span class="sxs-lookup"><span data-stu-id="35d96-276">Cyclical and Ordered content types are supported, but the algorithm treats them as discrete values and does not perform special processing.</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="35d96-277">См. также:</span><span class="sxs-lookup"><span data-stu-id="35d96-277">See Also</span></span>  
 <span data-ttu-id="35d96-278">[Алгоритм дерева принятия решений (Майкрософт)](microsoft-decision-trees-algorithm.md) </span><span class="sxs-lookup"><span data-stu-id="35d96-278">[Microsoft Decision Trees Algorithm](microsoft-decision-trees-algorithm.md) </span></span>  
 <span data-ttu-id="35d96-279">[Примеры запросов к модели дерева принятия решений](decision-trees-model-query-examples.md) </span><span class="sxs-lookup"><span data-stu-id="35d96-279">[Decision Trees Model Query Examples](decision-trees-model-query-examples.md) </span></span>  
 [<span data-ttu-id="35d96-280">Содержимое моделей интеллектуального анализа данных для моделей дерева принятия решений (службы Analysis Services — интеллектуальный анализ данных)</span><span class="sxs-lookup"><span data-stu-id="35d96-280">Mining Model Content for Decision Tree Models &#40;Analysis Services - Data Mining&#41;</span></span>](mining-model-content-for-decision-tree-models-analysis-services-data-mining.md)  
  
  
