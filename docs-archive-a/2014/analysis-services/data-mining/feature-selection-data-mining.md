---
title: Выбор компонентов (интеллектуальный анализ данных) | Документация Майкрософт
ms.custom: ''
ms.date: 03/06/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- mining models [Analysis Services], feature selections
- attributes [data mining]
- feature selection algorithms [Analysis Services]
- data mining [Analysis Services], feature selections
- neural network algorithms [Analysis Services]
- naive bayes algorithms [Analysis Services]
- decision tree algorithms [Analysis Services]
- datasets [Analysis Services]
- clustering algorithms [Analysis Services]
- coding [Data Mining]
ms.assetid: b044e785-4875-45ab-8ae4-cd3b4e3033bb
author: minewiskan
ms.author: owend
ms.openlocfilehash: ef5ee56636e7710074a893aed733905e1bf983bd
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 08/04/2020
ms.locfileid: "87665672"
---
# <a name="feature-selection-data-mining"></a>Выбор компонентов (интеллектуальный анализ данных)
  *Выбор компонентов* — термин, который обычно используется в интеллектуальном анализе данных для описания средств и методик, позволяющих сократить входные данные до управляемого размера для обработки и анализа. Выбор признаков подразумевает не только *уменьшение количества элементов*, то есть установленияую произвольную или предопределенную отработку количества атрибутов, которые могут быть учтены при построении модели, но также выбор атрибутов, что означает, что аналитик или инструмент моделирования активно выбирает или отменяет атрибуты на основе их полезности для анализа.  
  
 Возможность применения выбора компонентов имеет исключительную важность для эффективного анализа, поскольку наборы данных часто содержат больше информации, чем необходимо для построения модели. Например, в наборе данных может содержаться 500 столбцов, описывающих характеристики клиентов, но если данные в некоторых столбцах очень разрежены, то пользы от их добавления в модель будет немного. Если сохранить ненужные столбцы при построении модели, для ее обучения потребуется больше ресурсов ЦП и памяти, а для завершения модели – больше пространства хранения.  
  
 Даже при наличии больших ресурсов ненужные столбцы, как правило, удаляются, поскольку они могут снизить качество обнаруженных закономерностей по следующим причинам.  
  
-   Некоторые столбцы содержат зашумленные или избыточные данные. Из-за наличия шума затрудняется задача обнаружения значимых шаблонов в данных.  
  
-   Чтобы можно было обнаруживать более качественные шаблоны, для большинства алгоритмов интеллектуального анализа данных требуется намного более крупный набор данных для обучения на многомерном наборе данных. Но в некоторых приложениях интеллектуального анализа данных объем обучающих данных весьма мал.  
  
 Если лишь 50 из 500 столбцов в источнике данных содержат сведения, полезные для построения модели, можно отбросить лишние столбцы или воспользоваться методиками выбора компонентов для автоматического обнаружения оптимальных компонентов и исключения статистически незначимых значений. Выбор компонентов помогает решить дилемму: либо слишком много данных, имеющих небольшую ценность, либо слишком мало данных, имеющих высокую ценность.  
  
## <a name="feature-selection-in-analysis-services-data-mining"></a>Выбор компонентов при интеллектуальном анализе данных служб Analysis Services  
 Обычно выбор компонентов выполняется автоматически в службах [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)], а каждый алгоритм включает набор методик по умолчанию для интеллектуального сокращения количества компонентов. Выбор компонентов всегда выполняется до обучения модели, чтобы автоматически выбрать те атрибуты в наборе данных, которые с наибольшей вероятностью могут быть использованы в модели. Однако можно также вручную задать параметры, которые повлияют на порядок выбора компонентов.  
  
 Обычно выбор компонентов работает методом вычисления оценки для каждого атрибута, после чего осуществляется выбор только тех атрибутов, которые имеют наилучшие оценки. Предусмотрена также возможность коррекции пороговых значений для верхних оценок. В службах [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] предусмотрено множество методов для вычисления этих оценок. Конкретный метод, который будет применяться к модели, зависит от следующих факторов.  
  
-   Алгоритм, используемый в модели  
  
-   Тип данных атрибута  
  
-   Все параметры, которые можно задать в модели  
  
 Выбор компонентов применяется к входным данным, прогнозируемым атрибутам или состояниям в столбце. После того как оценка для выбора компонентов завершена, только те атрибуты и состояния, которые были выбраны алгоритмом, включаются в процесс построения модели и могут использоваться для прогноза. Если выбрать прогнозируемый атрибут, который не проходит порог для выбора компонентов, то он все же может использоваться для прогнозирования, но прогнозы будут основаны только на глобальной статистике, представленной в модели.  
  
> [!NOTE]  
>  Выбор компонентов затрагивает только те столбцы, которые используются в модели, и не влияет на хранилище структуры интеллектуального анализа данных. Столбцы, которые исключены из модели интеллектуального анализа данных, все еще остаются доступными в структуре, а данные в столбцах структуры интеллектуального анализа данных становятся кэшированными.  
  
### <a name="definition-of-feature-selection-methods"></a>Определение методов выбора компонентов  
 Существует много способов реализации выбора компонентов, в зависимости от типа данных, с которыми приходится работать, и от алгоритма, выбранного для анализа. В составе служб SQL Server Analysis Services предусмотрено несколько популярных и широко известных методов оценки атрибутов. Метод, применяемый в любом алгоритме или наборе данных, зависит от типов данных и использования столбцов.  
  
 Для ранжирования и сортировки атрибутов в столбцах, которые содержат недвоичные непрерывные числовые данные, используется оценка *интересность* .  
  
 Алгоритмы*Энтропия Шеннона* и оценки *Байеса* доступны для столбцов, содержащих дискретные и дискретизированные данные. Однако если модель содержит непрерывные столбцы, то оценка их полезности будет использоваться для оценки всех входных столбцов в целях обеспечения согласованности.  
  
 В следующем разделе приведено описание каждого метода выбора компонентов.  
  
#### <a name="interestingness-score"></a>Оценка интересности  
 Характеристика представляет интерес, если она предоставляет полезный фрагмент информации. Поскольку определение того, что полезно, зависит от сценария, отрасль интеллектуального анализа данных разработала различные способы измерения *интересности*. Например, *новизна* может представлять интерес при обнаружении выбросов, но возможность различения тесно связанных элементов или *различения веса*может быть более интересной для классификации.  
  
 Мера интересности, используемая в SQL Server Analysis Services, *основана на энтропии*, что означает, что атрибуты с произвольными распределениями имеют более высокую энтропию и получают более низкую информацию; Поэтому такие атрибуты менее интересны. Энтропия, относящаяся к любому конкретному атрибуту, сравнивается с энтропией всех других атрибутов следующим образом:  
  
 Интересность(Атрибут) = - (m - Энтропия(Атрибут)) * (m - Энтропия(Атрибут)).  
  
 Под главной энтропией, или m, подразумевается энтропия всего набора компонентов. Вычитая энтропию целевого атрибута из главной энтропии, можно оценить, сколько информации предоставляет атрибут.  
  
 Эта оценка используется по умолчанию каждый раз, когда столбец содержит недвоичные непрерывные числовые данные.  
  
#### <a name="shannons-entropy"></a>Энтропия Шеннона  
 Энтропия Шеннона используется для измерения неопределенности случайной переменной по отношению к конкретному результату. Например, энтропия броска монеты может быть представлена как функция вероятности выпадения орла.  
  
 В службах Analysis Services используется следующая формула для вычисления энтропии Шеннона:  
  
 H(X) = -∑ P(xi) log(P(xi))  
  
 Этот метод вычисления показателя доступен для дискретных и дискретизированных атрибутов.  
  
#### <a name="bayesian-with-k2-prior"></a>Алгоритм Байеса с априорной оценкой K2  
 В службах Analysis Services предусмотрены две оценки выбора компонентов, которые основаны на байесовских сетях. Байесовская сеть представляет собой *ориентированный* или *ациклический* граф состояний и переходов между состояниями; это означает, что некоторые состояния всегда предшествуют текущему состоянию, некоторые состояния следуют за ним, а граф не повторяется и не содержит циклов. По определению, байесовские сети обеспечивают использование априорных знаний. Но остается вопрос о том, какие предыдущие состояния должны использоваться при вычислении вероятностей последующих состояний, который важен с точки зрения проектирования, производительности и точности алгоритма.  
  
 Купером и Херсковицем был разработан алгоритм K2 для обучения на основе байесовской сети, который часто используется в интеллектуальном анализе данных. Он является масштабируемым и позволяет анализировать многочисленные переменные, но требует упорядочения переменных, используемых в качестве входных. Дополнительные сведения см. в статье Чикеринга, Гейгера и Хекермана [Обучаемые байесовские сети](https://go.microsoft.com/fwlink/?LinkId=105885) .  
  
 Этот метод вычисления показателя доступен для дискретных и дискретизированных атрибутов.  
  
#### <a name="bayesian-dirichlet-equivalent-with-uniform-prior"></a>Эквивалент Дирихле метода Байеса с однородной априорной оценкой  
 В оценке с помощью эквивалента Дирихле метода Байеса (BDE) также используется байесовский анализ для оценки сети на основе заданного набора данных. Метод оценки BDE был разработан Хекерманом и основан на методе BD, разработанном Купером и Херсковицем. Распределение Дирихле представляет собой мультиноминальное распределение, которое описывает условную вероятность каждой переменной в сети и имеет много свойств, полезных для обучения.  
  
 В методе, представляющем собой эквивалент Дирихле метода Байеса с однородной априорной оценкой (BDEU), предполагается наличие частного случая распределения Дирихле, в котором используется математическая константа для создания постоянного или равномерного распределения априорных состояний. В оценке BDE предполагается также эквивалентность правдоподобия, а это означает, что не следует ожидать, будто применяемые данные позволят различать эквивалентные структуры. Иными словами, если оценка для выражения "если А, то Б" аналогична оценке для выражения "если Б, то А", то соответствующие структуры нельзя различить на основе применяемых данных, поэтому не может быть сделан вывод о причинной обусловленности.  
  
 Дополнительные сведения о байесовских сетях и о реализации указанных методов оценки см. в статье [Обучаемые байесовские сети](https://go.microsoft.com/fwlink/?LinkId=105885).  
  
### <a name="feature-selection-methods-used-by-analysis-services-algorithms"></a>Методы выбора компонентов, используемые в алгоритмах служб Analysis Services  
 В следующей таблице перечислены алгоритмы, которые обеспечивают выбор компонентов, методы выбора компонентов, используемые в алгоритме, а также параметры, задаваемые в целях управления поведением при выборе компонентов.  
  
|Алгоритм|Метод анализа|Комментарии|  
|---------------|------------------------|--------------|  
|упрощенный алгоритм Байеса|Энтропия Шеннона<br /><br /> Алгоритм Байеса с априорной оценкой K2<br /><br /> Эквивалент Дирихле метода Байеса с однородной априорной оценкой (выбор по умолчанию)|В упрощенном алгоритме Байеса (Майкрософт) допускается применение только дискретных или дискретизированных атрибутов, поэтому в нем не может использоваться оценка интересности.<br /><br /> Дополнительные сведения об этом алгоритме см. в разделе [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md).|  
|Деревья принятия решений|Оценка интересности<br /><br /> Энтропия Шеннона<br /><br /> Алгоритм Байеса с априорной оценкой K2<br /><br /> Эквивалент Дирихле метода Байеса с однородной априорной оценкой (выбор по умолчанию)|Если какие-либо столбцы содержат недвоичные непрерывные значения, то оценка интересности используется для всех столбцов в целях обеспечения согласованности. В противном случае используется метод выбора компонентов по умолчанию или метод, указанный при создании модели.<br /><br /> Дополнительные сведения об этом алгоритме см. в разделе [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md).|  
|Нейронная сеть|Оценка интересности<br /><br /> Энтропия Шеннона<br /><br /> Алгоритм Байеса с априорной оценкой K2<br /><br /> Эквивалент Дирихле метода Байеса с однородной априорной оценкой (выбор по умолчанию)|В алгоритме нейронных сетей (Майкрософт) могут применяться оба метода: на основе энтропии и байесовских оценок — при условии, что данные содержат непрерывные столбцы.<br /><br /> Дополнительные сведения об этом алгоритме см. в разделе [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md).|  
|Логистическая регрессия|Оценка интересности<br /><br /> Энтропия Шеннона<br /><br /> Алгоритм Байеса с априорной оценкой K2<br /><br /> Эквивалент Дирихле метода Байеса с однородной априорной оценкой (выбор по умолчанию)|Хотя алгоритм логистической регрессии (Майкрософт) основан на алгоритме нейронной сети (Майкрософт), нельзя настроить модели логистической регрессии для управления поведением при выборе компонентов; поэтому по умолчанию выбор компонентов всегда выполняется методом, наиболее подходящим для атрибута.<br /><br /> Если все атрибуты являются дискретными или дискретизированными, то по умолчанию используется эквивалент Дирихле метода Байеса с однородной априорной оценкой (BDEU).<br /><br /> Дополнительные сведения об этом алгоритме см. в разделе [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md).|  
|Кластеризация|Оценка интересности|Алгоритм кластеризации (Майкрософт) может использовать дискретные или дискретизированные данные. Но поскольку оценка каждого атрибута вычисляется как расстояние и представляется числом из непрерывного ряда чисел, должна использоваться оценка интересности.<br /><br /> Дополнительные сведения об этом алгоритме см. в разделе [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md).|  
|Линейная регрессия|Оценка интересности|В алгоритме линейной регрессии (Майкрософт) применяется только оценка интересности, поскольку этот алгоритм поддерживает лишь непрерывные столбцы.<br /><br /> Дополнительные сведения об этом алгоритме см. в разделе [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md).|  
|Правила взаимосвязей<br /><br /> Кластеризация последовательностей|Не используется|Выбор компонентов не запускается с этими алгоритмами.<br /><br /> Тем не менее, можно управлять поведением алгоритма и, при необходимости, уменьшить размер входных данных, задавая значения параметров MINIMUM_SUPPORT и MINIMUM_PROBABILIITY.<br /><br /> Дополнительные сведения см. в разделах [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) и [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md).|  
|Временной ряд|Не используется|Выбор компонентов не применяется к моделям временных рядов.<br /><br /> Дополнительные сведения об этом алгоритме см. в разделе [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md).|  
  
## <a name="feature-selection-parameters"></a>Параметры выбора компонентов  
 Алгоритмы, поддерживающие выбор компонентов, позволяют управлять активностью выбора компонентов с помощью следующих параметров. В каждом алгоритме имеется заданное по умолчанию значение допустимого количества входов, но еще предоставляется возможность переопределить это значение по умолчанию и указать количество атрибутов. В этом разделе перечислены параметры для управления выбором компонентов.  
  
#### <a name="maximum_input_attributes"></a>MAXIMUM_INPUT_ATTRIBUTES  
 Если в модели содержится больше столбцов, чем задано в параметре *MAXIMUM_INPUT_ATTRIBUTES* , то алгоритм будет пропускать любые столбцы, не представляющие интереса с точки зрения выполненных им вычислений.  
  
#### <a name="maximum_output_attributes"></a>MAXIMUM_OUTPUT_ATTRIBUTES  
 Аналогичным образом, если в модели содержится больше прогнозируемых столбцов, чем задано в параметре *MAXIMUM_OUTPUT_ATTRIBUTES* , то алгоритм будет пропускать любые столбцы, не представляющие интереса с точки зрения выполненных им вычислений.  
  
#### <a name="maximum_states"></a>MAXIMUM_STATES  
 Если в модели содержится больше объектов, чем задано в параметре *MAXIMUM_STATES* , то наименее популярные состояния будут сводиться в одну группу и считаться отсутствующими. Если значение любого из данных параметров равно 0, то выбор компонентов отключается, что влияет на время обработки и производительность.  
  
 Помимо этих методов выбора компонентов, можно повысить эффективность алгоритма по определению или продвижению значимых атрибутов, задав *флаги моделирования* в модели либо *флаги распределения* в структуре. Дополнительные сведения об этих понятиях см. в разделах [Флаги моделирования (интеллектуальный анализ данных)](modeling-flags-data-mining.md) и [Распределения столбцов (интеллектуальный анализ данных)](column-distributions-data-mining.md)  
  
## <a name="see-also"></a>См. также:  
 [Настройка структуры и моделей интеллектуального анализа данных](customize-mining-models-and-structure.md)  
  
  
