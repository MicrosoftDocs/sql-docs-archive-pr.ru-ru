---
title: Выбор компонентов (интеллектуальный анализ данных) | Документация Майкрософт
ms.custom: ''
ms.date: 03/06/2017
ms.prod: sql-server-2014
ms.reviewer: ''
ms.technology: analysis-services
ms.topic: conceptual
helpviewer_keywords:
- mining models [Analysis Services], feature selections
- attributes [data mining]
- feature selection algorithms [Analysis Services]
- data mining [Analysis Services], feature selections
- neural network algorithms [Analysis Services]
- naive bayes algorithms [Analysis Services]
- decision tree algorithms [Analysis Services]
- datasets [Analysis Services]
- clustering algorithms [Analysis Services]
- coding [Data Mining]
ms.assetid: b044e785-4875-45ab-8ae4-cd3b4e3033bb
author: minewiskan
ms.author: owend
ms.openlocfilehash: ef5ee56636e7710074a893aed733905e1bf983bd
ms.sourcegitcommit: ad4d92dce894592a259721a1571b1d8736abacdb
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 08/04/2020
ms.locfileid: "87665672"
---
# <a name="feature-selection-data-mining"></a><span data-ttu-id="8676e-102">Выбор компонентов (интеллектуальный анализ данных)</span><span class="sxs-lookup"><span data-stu-id="8676e-102">Feature Selection (Data Mining)</span></span>
  <span data-ttu-id="8676e-103">*Выбор компонентов* — термин, который обычно используется в интеллектуальном анализе данных для описания средств и методик, позволяющих сократить входные данные до управляемого размера для обработки и анализа.</span><span class="sxs-lookup"><span data-stu-id="8676e-103">*Feature selection* is a term commonly used in data mining to describe the tools and techniques available for reducing inputs to a manageable size for processing and analysis.</span></span> <span data-ttu-id="8676e-104">Выбор признаков подразумевает не только *уменьшение количества элементов*, то есть установленияую произвольную или предопределенную отработку количества атрибутов, которые могут быть учтены при построении модели, но также выбор атрибутов, что означает, что аналитик или инструмент моделирования активно выбирает или отменяет атрибуты на основе их полезности для анализа.</span><span class="sxs-lookup"><span data-stu-id="8676e-104">Feature selection implies not only *cardinality reduction*, which means imposing an arbitrary or predefined cutoff on the number of attributes that can be considered when building a model, but also the choice of attributes, meaning that either the analyst or the modeling tool actively selects or discards attributes based on their usefulness for analysis.</span></span>  
  
 <span data-ttu-id="8676e-105">Возможность применения выбора компонентов имеет исключительную важность для эффективного анализа, поскольку наборы данных часто содержат больше информации, чем необходимо для построения модели.</span><span class="sxs-lookup"><span data-stu-id="8676e-105">The ability to apply feature selection is critical for effective analysis, because datasets frequently contain far more information than is needed to build the model.</span></span> <span data-ttu-id="8676e-106">Например, в наборе данных может содержаться 500 столбцов, описывающих характеристики клиентов, но если данные в некоторых столбцах очень разрежены, то пользы от их добавления в модель будет немного.</span><span class="sxs-lookup"><span data-stu-id="8676e-106">For example, a dataset might contain 500 columns that describe the characteristics of customers, but if the data in some of the columns is very sparse you would gain very little benefit from adding them to the model.</span></span> <span data-ttu-id="8676e-107">Если сохранить ненужные столбцы при построении модели, для ее обучения потребуется больше ресурсов ЦП и памяти, а для завершения модели – больше пространства хранения.</span><span class="sxs-lookup"><span data-stu-id="8676e-107">If you keep the unneeded columns while building the model, more CPU and memory are required during the training process, and more storage space is required for the completed model.</span></span>  
  
 <span data-ttu-id="8676e-108">Даже при наличии больших ресурсов ненужные столбцы, как правило, удаляются, поскольку они могут снизить качество обнаруженных закономерностей по следующим причинам.</span><span class="sxs-lookup"><span data-stu-id="8676e-108">Even if resources are not an issue, you typically want to remove unneeded columns because they might degrade the quality of discovered patterns, for the following reasons:</span></span>  
  
-   <span data-ttu-id="8676e-109">Некоторые столбцы содержат зашумленные или избыточные данные.</span><span class="sxs-lookup"><span data-stu-id="8676e-109">Some columns are noisy or redundant.</span></span> <span data-ttu-id="8676e-110">Из-за наличия шума затрудняется задача обнаружения значимых шаблонов в данных.</span><span class="sxs-lookup"><span data-stu-id="8676e-110">This noise makes it more difficult to discover meaningful patterns from the data;</span></span>  
  
-   <span data-ttu-id="8676e-111">Чтобы можно было обнаруживать более качественные шаблоны, для большинства алгоритмов интеллектуального анализа данных требуется намного более крупный набор данных для обучения на многомерном наборе данных.</span><span class="sxs-lookup"><span data-stu-id="8676e-111">To discover quality patterns, most data mining algorithms require much larger training data set on high-dimensional data set.</span></span> <span data-ttu-id="8676e-112">Но в некоторых приложениях интеллектуального анализа данных объем обучающих данных весьма мал.</span><span class="sxs-lookup"><span data-stu-id="8676e-112">But the training data is very small in some data mining applications.</span></span>  
  
 <span data-ttu-id="8676e-113">Если лишь 50 из 500 столбцов в источнике данных содержат сведения, полезные для построения модели, можно отбросить лишние столбцы или воспользоваться методиками выбора компонентов для автоматического обнаружения оптимальных компонентов и исключения статистически незначимых значений.</span><span class="sxs-lookup"><span data-stu-id="8676e-113">If only 50 of the 500 columns in the data source have information that is useful in building a model, you could just leave them out of the model, or you could use feature selection techniques to automatically discover the best features and to exclude values that are statistically insignificant.</span></span> <span data-ttu-id="8676e-114">Выбор компонентов помогает решить дилемму: либо слишком много данных, имеющих небольшую ценность, либо слишком мало данных, имеющих высокую ценность.</span><span class="sxs-lookup"><span data-stu-id="8676e-114">Feature selection helps solve the twin problems of having too much data that is of little value, or having too little data that is of high value.</span></span>  
  
## <a name="feature-selection-in-analysis-services-data-mining"></a><span data-ttu-id="8676e-115">Выбор компонентов при интеллектуальном анализе данных служб Analysis Services</span><span class="sxs-lookup"><span data-stu-id="8676e-115">Feature Selection in Analysis Services Data Mining</span></span>  
 <span data-ttu-id="8676e-116">Обычно выбор компонентов выполняется автоматически в службах [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)], а каждый алгоритм включает набор методик по умолчанию для интеллектуального сокращения количества компонентов.</span><span class="sxs-lookup"><span data-stu-id="8676e-116">Usually, feature selection is performed automatically in [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)], and each algorithm has a set of default techniques for intelligently applying feature reduction.</span></span> <span data-ttu-id="8676e-117">Выбор компонентов всегда выполняется до обучения модели, чтобы автоматически выбрать те атрибуты в наборе данных, которые с наибольшей вероятностью могут быть использованы в модели.</span><span class="sxs-lookup"><span data-stu-id="8676e-117">Feature selection is always performed before the model is trained, to automatically choose the attributes in a dataset that are most likely to be used in the model.</span></span> <span data-ttu-id="8676e-118">Однако можно также вручную задать параметры, которые повлияют на порядок выбора компонентов.</span><span class="sxs-lookup"><span data-stu-id="8676e-118">However, you can also manually set parameters to influence feature selection behavior.</span></span>  
  
 <span data-ttu-id="8676e-119">Обычно выбор компонентов работает методом вычисления оценки для каждого атрибута, после чего осуществляется выбор только тех атрибутов, которые имеют наилучшие оценки.</span><span class="sxs-lookup"><span data-stu-id="8676e-119">In general, feature selection works by calculating a score for each attribute, and then selecting only the attributes that have the best scores.</span></span> <span data-ttu-id="8676e-120">Предусмотрена также возможность коррекции пороговых значений для верхних оценок.</span><span class="sxs-lookup"><span data-stu-id="8676e-120">You can also adjust the threshold for the top scores.</span></span> <span data-ttu-id="8676e-121">В службах [!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] предусмотрено множество методов для вычисления этих оценок. Конкретный метод, который будет применяться к модели, зависит от следующих факторов.</span><span class="sxs-lookup"><span data-stu-id="8676e-121">[!INCLUDE[ssASnoversion](../../includes/ssasnoversion-md.md)] provides multiple methods for calculating these scores, and the exact method that is applied in any model depends on these factors:</span></span>  
  
-   <span data-ttu-id="8676e-122">Алгоритм, используемый в модели</span><span class="sxs-lookup"><span data-stu-id="8676e-122">The algorithm used in your model</span></span>  
  
-   <span data-ttu-id="8676e-123">Тип данных атрибута</span><span class="sxs-lookup"><span data-stu-id="8676e-123">The data type of the attribute</span></span>  
  
-   <span data-ttu-id="8676e-124">Все параметры, которые можно задать в модели</span><span class="sxs-lookup"><span data-stu-id="8676e-124">Any parameters that you may have set on your model</span></span>  
  
 <span data-ttu-id="8676e-125">Выбор компонентов применяется к входным данным, прогнозируемым атрибутам или состояниям в столбце.</span><span class="sxs-lookup"><span data-stu-id="8676e-125">Feature selection is applied to inputs, predictable attributes, or to states in a column.</span></span> <span data-ttu-id="8676e-126">После того как оценка для выбора компонентов завершена, только те атрибуты и состояния, которые были выбраны алгоритмом, включаются в процесс построения модели и могут использоваться для прогноза.</span><span class="sxs-lookup"><span data-stu-id="8676e-126">When scoring for feature selection is complete, only the attributes and states that the algorithm selects are included in the model-building process and can be used for prediction.</span></span> <span data-ttu-id="8676e-127">Если выбрать прогнозируемый атрибут, который не проходит порог для выбора компонентов, то он все же может использоваться для прогнозирования, но прогнозы будут основаны только на глобальной статистике, представленной в модели.</span><span class="sxs-lookup"><span data-stu-id="8676e-127">If you choose a predictable attribute that does not meet the threshold for feature selection the attribute can still be used for prediction, but the predictions will be based solely on the global statistics that exist in the model.</span></span>  
  
> [!NOTE]  
>  <span data-ttu-id="8676e-128">Выбор компонентов затрагивает только те столбцы, которые используются в модели, и не влияет на хранилище структуры интеллектуального анализа данных.</span><span class="sxs-lookup"><span data-stu-id="8676e-128">Feature selection affects only the columns that are used in the model, and has no effect on storage of the mining structure.</span></span> <span data-ttu-id="8676e-129">Столбцы, которые исключены из модели интеллектуального анализа данных, все еще остаются доступными в структуре, а данные в столбцах структуры интеллектуального анализа данных становятся кэшированными.</span><span class="sxs-lookup"><span data-stu-id="8676e-129">The columns that you leave out of the mining model are still available in the structure, and data in the mining structure columns will be cached.</span></span>  
  
### <a name="definition-of-feature-selection-methods"></a><span data-ttu-id="8676e-130">Определение методов выбора компонентов</span><span class="sxs-lookup"><span data-stu-id="8676e-130">Definition of Feature Selection Methods</span></span>  
 <span data-ttu-id="8676e-131">Существует много способов реализации выбора компонентов, в зависимости от типа данных, с которыми приходится работать, и от алгоритма, выбранного для анализа.</span><span class="sxs-lookup"><span data-stu-id="8676e-131">There are many ways to implement feature selection, depending on the type of data that you are working with and the algorithm that you choose for analysis.</span></span> <span data-ttu-id="8676e-132">В составе служб SQL Server Analysis Services предусмотрено несколько популярных и широко известных методов оценки атрибутов.</span><span class="sxs-lookup"><span data-stu-id="8676e-132">SQL Server Analysis Services provides several popular and well-established methods for scoring attributes.</span></span> <span data-ttu-id="8676e-133">Метод, применяемый в любом алгоритме или наборе данных, зависит от типов данных и использования столбцов.</span><span class="sxs-lookup"><span data-stu-id="8676e-133">The method that is applied in any algorithm or data set depends on the data types, and the column usage.</span></span>  
  
 <span data-ttu-id="8676e-134">Для ранжирования и сортировки атрибутов в столбцах, которые содержат недвоичные непрерывные числовые данные, используется оценка *интересность* .</span><span class="sxs-lookup"><span data-stu-id="8676e-134">The *interestingness* score is used to rank and sort attributes in columns that contain nonbinary continuous numeric data.</span></span>  
  
 <span data-ttu-id="8676e-135">Алгоритмы*Энтропия Шеннона* и оценки *Байеса* доступны для столбцов, содержащих дискретные и дискретизированные данные.</span><span class="sxs-lookup"><span data-stu-id="8676e-135">*Shannon's entropy* and two *Bayesian* scores are available for columns that contain discrete and discretized data.</span></span> <span data-ttu-id="8676e-136">Однако если модель содержит непрерывные столбцы, то оценка их полезности будет использоваться для оценки всех входных столбцов в целях обеспечения согласованности.</span><span class="sxs-lookup"><span data-stu-id="8676e-136">However, if the model contains any continuous columns, the interestingness score will be used to assess all input columns, to ensure consistency.</span></span>  
  
 <span data-ttu-id="8676e-137">В следующем разделе приведено описание каждого метода выбора компонентов.</span><span class="sxs-lookup"><span data-stu-id="8676e-137">The following section describes each method of feature selection.</span></span>  
  
#### <a name="interestingness-score"></a><span data-ttu-id="8676e-138">Оценка интересности</span><span class="sxs-lookup"><span data-stu-id="8676e-138">Interestingness score</span></span>  
 <span data-ttu-id="8676e-139">Характеристика представляет интерес, если она предоставляет полезный фрагмент информации.</span><span class="sxs-lookup"><span data-stu-id="8676e-139">A feature is interesting if it tells you some useful piece of information.</span></span> <span data-ttu-id="8676e-140">Поскольку определение того, что полезно, зависит от сценария, отрасль интеллектуального анализа данных разработала различные способы измерения *интересности*.</span><span class="sxs-lookup"><span data-stu-id="8676e-140">Because the definition of what is useful varies depending on the scenario, the data mining industry has developed various ways to measure *interestingness*.</span></span> <span data-ttu-id="8676e-141">Например, *новизна* может представлять интерес при обнаружении выбросов, но возможность различения тесно связанных элементов или *различения веса*может быть более интересной для классификации.</span><span class="sxs-lookup"><span data-stu-id="8676e-141">For example, *novelty* might be interesting in outlier detection, but the ability to discriminate between closely related items, or *discriminating weight*, might be more interesting for classification.</span></span>  
  
 <span data-ttu-id="8676e-142">Мера интересности, используемая в SQL Server Analysis Services, *основана на энтропии*, что означает, что атрибуты с произвольными распределениями имеют более высокую энтропию и получают более низкую информацию; Поэтому такие атрибуты менее интересны.</span><span class="sxs-lookup"><span data-stu-id="8676e-142">The measure of interestingness that is used in SQL Server Analysis Services is *entropy-based*, meaning that attributes with random distributions have higher entropy and lower information gain; therefore, such attributes are less interesting.</span></span> <span data-ttu-id="8676e-143">Энтропия, относящаяся к любому конкретному атрибуту, сравнивается с энтропией всех других атрибутов следующим образом:</span><span class="sxs-lookup"><span data-stu-id="8676e-143">The entropy for any particular attribute is compared to the entropy of all other attributes, as follows:</span></span>  
  
 <span data-ttu-id="8676e-144">Интересность(Атрибут) = - (m - Энтропия(Атрибут)) \* (m - Энтропия(Атрибут)).</span><span class="sxs-lookup"><span data-stu-id="8676e-144">Interestingness(Attribute) = - (m - Entropy(Attribute)) \* (m - Entropy(Attribute))</span></span>  
  
 <span data-ttu-id="8676e-145">Под главной энтропией, или m, подразумевается энтропия всего набора компонентов.</span><span class="sxs-lookup"><span data-stu-id="8676e-145">Central entropy, or m, means the entropy of the entire feature set.</span></span> <span data-ttu-id="8676e-146">Вычитая энтропию целевого атрибута из главной энтропии, можно оценить, сколько информации предоставляет атрибут.</span><span class="sxs-lookup"><span data-stu-id="8676e-146">By subtracting the entropy of the target attribute from the central entropy, you can assess how much information the attribute provides.</span></span>  
  
 <span data-ttu-id="8676e-147">Эта оценка используется по умолчанию каждый раз, когда столбец содержит недвоичные непрерывные числовые данные.</span><span class="sxs-lookup"><span data-stu-id="8676e-147">This score is used by default whenever the column contains nonbinary continuous numeric data.</span></span>  
  
#### <a name="shannons-entropy"></a><span data-ttu-id="8676e-148">Энтропия Шеннона</span><span class="sxs-lookup"><span data-stu-id="8676e-148">Shannon's Entropy</span></span>  
 <span data-ttu-id="8676e-149">Энтропия Шеннона используется для измерения неопределенности случайной переменной по отношению к конкретному результату.</span><span class="sxs-lookup"><span data-stu-id="8676e-149">Shannon's entropy measures the uncertainty of a random variable for a particular outcome.</span></span> <span data-ttu-id="8676e-150">Например, энтропия броска монеты может быть представлена как функция вероятности выпадения орла.</span><span class="sxs-lookup"><span data-stu-id="8676e-150">For example, the entropy of a coin toss can be represented as a function of the probability of it coming up heads.</span></span>  
  
 <span data-ttu-id="8676e-151">В службах Analysis Services используется следующая формула для вычисления энтропии Шеннона:</span><span class="sxs-lookup"><span data-stu-id="8676e-151">Analysis Services uses the following formula to calculate Shannon's entropy:</span></span>  
  
 <span data-ttu-id="8676e-152">H(X) = -∑ P(xi) log(P(xi))</span><span class="sxs-lookup"><span data-stu-id="8676e-152">H(X) = -∑ P(xi) log(P(xi))</span></span>  
  
 <span data-ttu-id="8676e-153">Этот метод вычисления показателя доступен для дискретных и дискретизированных атрибутов.</span><span class="sxs-lookup"><span data-stu-id="8676e-153">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-with-k2-prior"></a><span data-ttu-id="8676e-154">Алгоритм Байеса с априорной оценкой K2</span><span class="sxs-lookup"><span data-stu-id="8676e-154">Bayesian with K2 Prior</span></span>  
 <span data-ttu-id="8676e-155">В службах Analysis Services предусмотрены две оценки выбора компонентов, которые основаны на байесовских сетях.</span><span class="sxs-lookup"><span data-stu-id="8676e-155">Analysis Services provides two feature selection scores that are based on Bayesian networks.</span></span> <span data-ttu-id="8676e-156">Байесовская сеть представляет собой *ориентированный* или *ациклический* граф состояний и переходов между состояниями; это означает, что некоторые состояния всегда предшествуют текущему состоянию, некоторые состояния следуют за ним, а граф не повторяется и не содержит циклов.</span><span class="sxs-lookup"><span data-stu-id="8676e-156">A Bayesian network is a *directed* or *acyclic* graph of states and transitions between states, meaning that some states are always prior to the current state, some states are posterior, and the graph does not repeat or loop.</span></span> <span data-ttu-id="8676e-157">По определению, байесовские сети обеспечивают использование априорных знаний.</span><span class="sxs-lookup"><span data-stu-id="8676e-157">By definition, Bayesian networks allow the use of prior knowledge.</span></span> <span data-ttu-id="8676e-158">Но остается вопрос о том, какие предыдущие состояния должны использоваться при вычислении вероятностей последующих состояний, который важен с точки зрения проектирования, производительности и точности алгоритма.</span><span class="sxs-lookup"><span data-stu-id="8676e-158">However, the question of which prior states to use in calculating probabilities of later states is important for algorithm design, performance, and accuracy.</span></span>  
  
 <span data-ttu-id="8676e-159">Купером и Херсковицем был разработан алгоритм K2 для обучения на основе байесовской сети, который часто используется в интеллектуальном анализе данных.</span><span class="sxs-lookup"><span data-stu-id="8676e-159">The K2 algorithm for learning from a Bayesian network was developed by Cooper and Herskovits and is often used in data mining.</span></span> <span data-ttu-id="8676e-160">Он является масштабируемым и позволяет анализировать многочисленные переменные, но требует упорядочения переменных, используемых в качестве входных.</span><span class="sxs-lookup"><span data-stu-id="8676e-160">It is scalable and can analyze multiple variables, but requires ordering on variables used as input.</span></span> <span data-ttu-id="8676e-161">Дополнительные сведения см. в статье Чикеринга, Гейгера и Хекермана [Обучаемые байесовские сети](https://go.microsoft.com/fwlink/?LinkId=105885) .</span><span class="sxs-lookup"><span data-stu-id="8676e-161">For more information, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885) by Chickering, Geiger, and Heckerman.</span></span>  
  
 <span data-ttu-id="8676e-162">Этот метод вычисления показателя доступен для дискретных и дискретизированных атрибутов.</span><span class="sxs-lookup"><span data-stu-id="8676e-162">This scoring method is available for discrete and discretized attributes.</span></span>  
  
#### <a name="bayesian-dirichlet-equivalent-with-uniform-prior"></a><span data-ttu-id="8676e-163">Эквивалент Дирихле метода Байеса с однородной априорной оценкой</span><span class="sxs-lookup"><span data-stu-id="8676e-163">Bayesian Dirichlet Equivalent with Uniform Prior</span></span>  
 <span data-ttu-id="8676e-164">В оценке с помощью эквивалента Дирихле метода Байеса (BDE) также используется байесовский анализ для оценки сети на основе заданного набора данных.</span><span class="sxs-lookup"><span data-stu-id="8676e-164">The Bayesian Dirichlet Equivalent (BDE) score also uses Bayesian analysis to evaluate a network given a dataset.</span></span> <span data-ttu-id="8676e-165">Метод оценки BDE был разработан Хекерманом и основан на методе BD, разработанном Купером и Херсковицем.</span><span class="sxs-lookup"><span data-stu-id="8676e-165">The BDE scoring method was developed by Heckerman and is based on the BD metric developed by Cooper and Herskovits.</span></span> <span data-ttu-id="8676e-166">Распределение Дирихле представляет собой мультиноминальное распределение, которое описывает условную вероятность каждой переменной в сети и имеет много свойств, полезных для обучения.</span><span class="sxs-lookup"><span data-stu-id="8676e-166">The Dirichlet distribution is a multinomial distribution that describes the conditional probability of each variable in the network, and has many properties that are useful for learning.</span></span>  
  
 <span data-ttu-id="8676e-167">В методе, представляющем собой эквивалент Дирихле метода Байеса с однородной априорной оценкой (BDEU), предполагается наличие частного случая распределения Дирихле, в котором используется математическая константа для создания постоянного или равномерного распределения априорных состояний.</span><span class="sxs-lookup"><span data-stu-id="8676e-167">The Bayesian Dirichlet Equivalent with Uniform Prior (BDEU) method assumes a special case of the Dirichlet distribution, in which a mathematical constant is used to create a fixed or uniform distribution of prior states.</span></span> <span data-ttu-id="8676e-168">В оценке BDE предполагается также эквивалентность правдоподобия, а это означает, что не следует ожидать, будто применяемые данные позволят различать эквивалентные структуры.</span><span class="sxs-lookup"><span data-stu-id="8676e-168">The BDE score also assumes likelihood equivalence, which means that the data cannot be expected to discriminate equivalent structures.</span></span> <span data-ttu-id="8676e-169">Иными словами, если оценка для выражения "если А, то Б" аналогична оценке для выражения "если Б, то А", то соответствующие структуры нельзя различить на основе применяемых данных, поэтому не может быть сделан вывод о причинной обусловленности.</span><span class="sxs-lookup"><span data-stu-id="8676e-169">In other words, if the score for If A Then B is the same as the score for If B Then A, the structures cannot be distinguished based on the data, and causation cannot be inferred.</span></span>  
  
 <span data-ttu-id="8676e-170">Дополнительные сведения о байесовских сетях и о реализации указанных методов оценки см. в статье [Обучаемые байесовские сети](https://go.microsoft.com/fwlink/?LinkId=105885).</span><span class="sxs-lookup"><span data-stu-id="8676e-170">For more information about Bayesian networks and the implementation of these scoring methods, see [Learning Bayesian Networks](https://go.microsoft.com/fwlink/?LinkId=105885).</span></span>  
  
### <a name="feature-selection-methods-used-by-analysis-services-algorithms"></a><span data-ttu-id="8676e-171">Методы выбора компонентов, используемые в алгоритмах служб Analysis Services</span><span class="sxs-lookup"><span data-stu-id="8676e-171">Feature Selection Methods used by Analysis Services Algorithms</span></span>  
 <span data-ttu-id="8676e-172">В следующей таблице перечислены алгоритмы, которые обеспечивают выбор компонентов, методы выбора компонентов, используемые в алгоритме, а также параметры, задаваемые в целях управления поведением при выборе компонентов.</span><span class="sxs-lookup"><span data-stu-id="8676e-172">The following table lists the algorithms that support feature selection, the feature selection methods used by the algorithm, and the parameters that you set to control feature selection behavior:</span></span>  
  
|<span data-ttu-id="8676e-173">Алгоритм</span><span class="sxs-lookup"><span data-stu-id="8676e-173">Algorithm</span></span>|<span data-ttu-id="8676e-174">Метод анализа</span><span class="sxs-lookup"><span data-stu-id="8676e-174">Method of analysis</span></span>|<span data-ttu-id="8676e-175">Комментарии</span><span class="sxs-lookup"><span data-stu-id="8676e-175">Comments</span></span>|  
|---------------|------------------------|--------------|  
|<span data-ttu-id="8676e-176">упрощенный алгоритм Байеса</span><span class="sxs-lookup"><span data-stu-id="8676e-176">Naive Bayes</span></span>|<span data-ttu-id="8676e-177">Энтропия Шеннона</span><span class="sxs-lookup"><span data-stu-id="8676e-177">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="8676e-178">Алгоритм Байеса с априорной оценкой K2</span><span class="sxs-lookup"><span data-stu-id="8676e-178">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="8676e-179">Эквивалент Дирихле метода Байеса с однородной априорной оценкой (выбор по умолчанию)</span><span class="sxs-lookup"><span data-stu-id="8676e-179">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="8676e-180">В упрощенном алгоритме Байеса (Майкрософт) допускается применение только дискретных или дискретизированных атрибутов, поэтому в нем не может использоваться оценка интересности.</span><span class="sxs-lookup"><span data-stu-id="8676e-180">The Microsoft Naïve Bayes algorithm accepts only discrete or discretized attributes; therefore, it cannot use the interestingness score.</span></span><br /><br /> <span data-ttu-id="8676e-181">Дополнительные сведения об этом алгоритме см. в разделе [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="8676e-181">For more information about this algorithm, see [Microsoft Naive Bayes Algorithm Technical Reference](microsoft-naive-bayes-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="8676e-182">Деревья принятия решений</span><span class="sxs-lookup"><span data-stu-id="8676e-182">Decision trees</span></span>|<span data-ttu-id="8676e-183">Оценка интересности</span><span class="sxs-lookup"><span data-stu-id="8676e-183">Interestingness score</span></span><br /><br /> <span data-ttu-id="8676e-184">Энтропия Шеннона</span><span class="sxs-lookup"><span data-stu-id="8676e-184">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="8676e-185">Алгоритм Байеса с априорной оценкой K2</span><span class="sxs-lookup"><span data-stu-id="8676e-185">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="8676e-186">Эквивалент Дирихле метода Байеса с однородной априорной оценкой (выбор по умолчанию)</span><span class="sxs-lookup"><span data-stu-id="8676e-186">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="8676e-187">Если какие-либо столбцы содержат недвоичные непрерывные значения, то оценка интересности используется для всех столбцов в целях обеспечения согласованности.</span><span class="sxs-lookup"><span data-stu-id="8676e-187">If any columns contain non-binary continuous values, the interestingness score is used for all columns, to ensure consistency.</span></span> <span data-ttu-id="8676e-188">В противном случае используется метод выбора компонентов по умолчанию или метод, указанный при создании модели.</span><span class="sxs-lookup"><span data-stu-id="8676e-188">Otherwise, the default feature selection method is used, or the method that you specified when you created the model.</span></span><br /><br /> <span data-ttu-id="8676e-189">Дополнительные сведения об этом алгоритме см. в разделе [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="8676e-189">For more information about this algorithm, see [Microsoft Decision Trees Algorithm Technical Reference](microsoft-decision-trees-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="8676e-190">Нейронная сеть</span><span class="sxs-lookup"><span data-stu-id="8676e-190">Neural network</span></span>|<span data-ttu-id="8676e-191">Оценка интересности</span><span class="sxs-lookup"><span data-stu-id="8676e-191">Interestingness score</span></span><br /><br /> <span data-ttu-id="8676e-192">Энтропия Шеннона</span><span class="sxs-lookup"><span data-stu-id="8676e-192">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="8676e-193">Алгоритм Байеса с априорной оценкой K2</span><span class="sxs-lookup"><span data-stu-id="8676e-193">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="8676e-194">Эквивалент Дирихле метода Байеса с однородной априорной оценкой (выбор по умолчанию)</span><span class="sxs-lookup"><span data-stu-id="8676e-194">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="8676e-195">В алгоритме нейронных сетей (Майкрософт) могут применяться оба метода: на основе энтропии и байесовских оценок — при условии, что данные содержат непрерывные столбцы.</span><span class="sxs-lookup"><span data-stu-id="8676e-195">The Microsoft Neural Networks algorithm can use both Bayesian and entropy-based methods, as long as the data contains continuous columns.</span></span><br /><br /> <span data-ttu-id="8676e-196">Дополнительные сведения об этом алгоритме см. в разделе [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="8676e-196">For more information about this algorithm, see [Microsoft Neural Network Algorithm Technical Reference](microsoft-neural-network-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="8676e-197">Логистическая регрессия</span><span class="sxs-lookup"><span data-stu-id="8676e-197">Logistic regression</span></span>|<span data-ttu-id="8676e-198">Оценка интересности</span><span class="sxs-lookup"><span data-stu-id="8676e-198">Interestingness score</span></span><br /><br /> <span data-ttu-id="8676e-199">Энтропия Шеннона</span><span class="sxs-lookup"><span data-stu-id="8676e-199">Shannon's Entropy</span></span><br /><br /> <span data-ttu-id="8676e-200">Алгоритм Байеса с априорной оценкой K2</span><span class="sxs-lookup"><span data-stu-id="8676e-200">Bayesian with K2 Prior</span></span><br /><br /> <span data-ttu-id="8676e-201">Эквивалент Дирихле метода Байеса с однородной априорной оценкой (выбор по умолчанию)</span><span class="sxs-lookup"><span data-stu-id="8676e-201">Bayesian Dirichlet with uniform prior (default)</span></span>|<span data-ttu-id="8676e-202">Хотя алгоритм логистической регрессии (Майкрософт) основан на алгоритме нейронной сети (Майкрософт), нельзя настроить модели логистической регрессии для управления поведением при выборе компонентов; поэтому по умолчанию выбор компонентов всегда выполняется методом, наиболее подходящим для атрибута.</span><span class="sxs-lookup"><span data-stu-id="8676e-202">Although the Microsoft Logistic Regression algorithm is based on the Microsoft Neural Network algorithm, you cannot customize logistic regression models to control feature selection behavior; therefore, feature selection always default to the method that is most appropriate for the attribute.</span></span><br /><br /> <span data-ttu-id="8676e-203">Если все атрибуты являются дискретными или дискретизированными, то по умолчанию используется эквивалент Дирихле метода Байеса с однородной априорной оценкой (BDEU).</span><span class="sxs-lookup"><span data-stu-id="8676e-203">If all attributes are discrete or discretized, the default is BDEU.</span></span><br /><br /> <span data-ttu-id="8676e-204">Дополнительные сведения об этом алгоритме см. в разделе [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="8676e-204">For more information about this algorithm, see [Microsoft Logistic Regression Algorithm Technical Reference](microsoft-logistic-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="8676e-205">Кластеризация</span><span class="sxs-lookup"><span data-stu-id="8676e-205">Clustering</span></span>|<span data-ttu-id="8676e-206">Оценка интересности</span><span class="sxs-lookup"><span data-stu-id="8676e-206">Interestingness score</span></span>|<span data-ttu-id="8676e-207">Алгоритм кластеризации (Майкрософт) может использовать дискретные или дискретизированные данные.</span><span class="sxs-lookup"><span data-stu-id="8676e-207">The Microsoft Clustering algorithm can use discrete or discretized data.</span></span> <span data-ttu-id="8676e-208">Но поскольку оценка каждого атрибута вычисляется как расстояние и представляется числом из непрерывного ряда чисел, должна использоваться оценка интересности.</span><span class="sxs-lookup"><span data-stu-id="8676e-208">However, because the score of each attribute is calculated as a distance and is represented as a continuous number, the interestingness score must be used.</span></span><br /><br /> <span data-ttu-id="8676e-209">Дополнительные сведения об этом алгоритме см. в разделе [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="8676e-209">For more information about this algorithm, see [Microsoft Clustering Algorithm Technical Reference](microsoft-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="8676e-210">Линейная регрессия</span><span class="sxs-lookup"><span data-stu-id="8676e-210">Linear regression</span></span>|<span data-ttu-id="8676e-211">Оценка интересности</span><span class="sxs-lookup"><span data-stu-id="8676e-211">Interestingness score</span></span>|<span data-ttu-id="8676e-212">В алгоритме линейной регрессии (Майкрософт) применяется только оценка интересности, поскольку этот алгоритм поддерживает лишь непрерывные столбцы.</span><span class="sxs-lookup"><span data-stu-id="8676e-212">The Microsoft Linear Regression algorithm can only use the interestingness score, because it only supports continuous columns.</span></span><br /><br /> <span data-ttu-id="8676e-213">Дополнительные сведения об этом алгоритме см. в разделе [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="8676e-213">For more information about this algorithm, see [Microsoft Linear Regression Algorithm Technical Reference](microsoft-linear-regression-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="8676e-214">Правила взаимосвязей</span><span class="sxs-lookup"><span data-stu-id="8676e-214">Association rules</span></span><br /><br /> <span data-ttu-id="8676e-215">Кластеризация последовательностей</span><span class="sxs-lookup"><span data-stu-id="8676e-215">Sequence clustering</span></span>|<span data-ttu-id="8676e-216">Не используется</span><span class="sxs-lookup"><span data-stu-id="8676e-216">Not used</span></span>|<span data-ttu-id="8676e-217">Выбор компонентов не запускается с этими алгоритмами.</span><span class="sxs-lookup"><span data-stu-id="8676e-217">Feature selection is not invoked with these algorithms.</span></span><br /><br /> <span data-ttu-id="8676e-218">Тем не менее, можно управлять поведением алгоритма и, при необходимости, уменьшить размер входных данных, задавая значения параметров MINIMUM_SUPPORT и MINIMUM_PROBABILIITY.</span><span class="sxs-lookup"><span data-stu-id="8676e-218">However, you can control the behavior of the algorithm and reduce the size of input data if necessary by setting the value of the parameters MINIMUM_SUPPORT and MINIMUM_PROBABILIITY.</span></span><br /><br /> <span data-ttu-id="8676e-219">Дополнительные сведения см. в разделах [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) и [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="8676e-219">For more information, see [Microsoft Association Algorithm Technical Reference](microsoft-association-algorithm-technical-reference.md) and [Microsoft Sequence Clustering Algorithm Technical Reference](microsoft-sequence-clustering-algorithm-technical-reference.md).</span></span>|  
|<span data-ttu-id="8676e-220">Временной ряд</span><span class="sxs-lookup"><span data-stu-id="8676e-220">Time series</span></span>|<span data-ttu-id="8676e-221">Не используется</span><span class="sxs-lookup"><span data-stu-id="8676e-221">Not used</span></span>|<span data-ttu-id="8676e-222">Выбор компонентов не применяется к моделям временных рядов.</span><span class="sxs-lookup"><span data-stu-id="8676e-222">Feature selection does not apply to time series models.</span></span><br /><br /> <span data-ttu-id="8676e-223">Дополнительные сведения об этом алгоритме см. в разделе [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md).</span><span class="sxs-lookup"><span data-stu-id="8676e-223">For more information about this algorithm, see [Microsoft Time Series Algorithm Technical Reference](microsoft-time-series-algorithm-technical-reference.md).</span></span>|  
  
## <a name="feature-selection-parameters"></a><span data-ttu-id="8676e-224">Параметры выбора компонентов</span><span class="sxs-lookup"><span data-stu-id="8676e-224">Feature Selection Parameters</span></span>  
 <span data-ttu-id="8676e-225">Алгоритмы, поддерживающие выбор компонентов, позволяют управлять активностью выбора компонентов с помощью следующих параметров.</span><span class="sxs-lookup"><span data-stu-id="8676e-225">In algorithms that support feature selection, you can control when feature selection is turned on by using the following parameters.</span></span> <span data-ttu-id="8676e-226">В каждом алгоритме имеется заданное по умолчанию значение допустимого количества входов, но еще предоставляется возможность переопределить это значение по умолчанию и указать количество атрибутов.</span><span class="sxs-lookup"><span data-stu-id="8676e-226">Each algorithm has a default value for the number of inputs that are allowed, but you can override this default and specify the number of attributes.</span></span> <span data-ttu-id="8676e-227">В этом разделе перечислены параметры для управления выбором компонентов.</span><span class="sxs-lookup"><span data-stu-id="8676e-227">This section lists the parameters that are provided for managing feature selection.</span></span>  
  
#### <a name="maximum_input_attributes"></a><span data-ttu-id="8676e-228">MAXIMUM_INPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="8676e-228">MAXIMUM_INPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="8676e-229">Если в модели содержится больше столбцов, чем задано в параметре *MAXIMUM_INPUT_ATTRIBUTES* , то алгоритм будет пропускать любые столбцы, не представляющие интереса с точки зрения выполненных им вычислений.</span><span class="sxs-lookup"><span data-stu-id="8676e-229">If a model contains more columns than the number that is specified in the *MAXIMUM_INPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_output_attributes"></a><span data-ttu-id="8676e-230">MAXIMUM_OUTPUT_ATTRIBUTES</span><span class="sxs-lookup"><span data-stu-id="8676e-230">MAXIMUM_OUTPUT_ATTRIBUTES</span></span>  
 <span data-ttu-id="8676e-231">Аналогичным образом, если в модели содержится больше прогнозируемых столбцов, чем задано в параметре *MAXIMUM_OUTPUT_ATTRIBUTES* , то алгоритм будет пропускать любые столбцы, не представляющие интереса с точки зрения выполненных им вычислений.</span><span class="sxs-lookup"><span data-stu-id="8676e-231">Similarly, if a model contains more predictable columns than the number that is specified in the *MAXIMUM_OUTPUT_ATTRIBUTES* parameter, the algorithm ignores any columns that it calculates to be uninteresting.</span></span>  
  
#### <a name="maximum_states"></a><span data-ttu-id="8676e-232">MAXIMUM_STATES</span><span class="sxs-lookup"><span data-stu-id="8676e-232">MAXIMUM_STATES</span></span>  
 <span data-ttu-id="8676e-233">Если в модели содержится больше объектов, чем задано в параметре *MAXIMUM_STATES* , то наименее популярные состояния будут сводиться в одну группу и считаться отсутствующими.</span><span class="sxs-lookup"><span data-stu-id="8676e-233">If a model contains more cases than are specified in the *MAXIMUM_STATES* parameter, the least popular states are grouped together and treated as missing.</span></span> <span data-ttu-id="8676e-234">Если значение любого из данных параметров равно 0, то выбор компонентов отключается, что влияет на время обработки и производительность.</span><span class="sxs-lookup"><span data-stu-id="8676e-234">If any one of these parameters is set to 0, feature selection is turned off, affecting processing time and performance.</span></span>  
  
 <span data-ttu-id="8676e-235">Помимо этих методов выбора компонентов, можно повысить эффективность алгоритма по определению или продвижению значимых атрибутов, задав *флаги моделирования* в модели либо *флаги распределения* в структуре.</span><span class="sxs-lookup"><span data-stu-id="8676e-235">In addition to these methods for feature selection, you can improve the ability of the algorithm to identify or promote meaningful attributes by setting *modeling flags* on the model or by setting *distribution flags* on the structure.</span></span> <span data-ttu-id="8676e-236">Дополнительные сведения об этих понятиях см. в разделах [Флаги моделирования (интеллектуальный анализ данных)](modeling-flags-data-mining.md) и [Распределения столбцов (интеллектуальный анализ данных)](column-distributions-data-mining.md)</span><span class="sxs-lookup"><span data-stu-id="8676e-236">For more information about these concepts, see [Modeling Flags &#40;Data Mining&#41;](modeling-flags-data-mining.md) and [Column Distributions &#40;Data Mining&#41;](column-distributions-data-mining.md).</span></span>  
  
## <a name="see-also"></a><span data-ttu-id="8676e-237">См. также:</span><span class="sxs-lookup"><span data-stu-id="8676e-237">See Also</span></span>  
 [<span data-ttu-id="8676e-238">Настройка структуры и моделей интеллектуального анализа данных</span><span class="sxs-lookup"><span data-stu-id="8676e-238">Customize Mining Models and Structure</span></span>](customize-mining-models-and-structure.md)  
  
  
